program(1.0)
[buildInfo = dict<tensor<string, []>, tensor<string, []>>({{"coremlc-component-MIL", "3400.17.1"}, {"coremlc-version", "3400.17.1"}})]
{
    func main<ios16>(tensor<fp32, [1, 512]> attention_mask, tensor<fp32, [1, 512]> input_ids) {
            tensor<fp32, [30522, 128]> electra_embeddings_word_embeddings_weight_affine_quantized = constexpr_affine_dequantize()[axis = tensor<int32, []>(0), name = tensor<string, []>("electra_embeddings_word_embeddings_weight_affine_quantized"), quantized_data = tensor<int8, [30522, 128]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(64))), scale = tensor<fp32, [30522]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(3937536))), zero_point = tensor<int8, [30522]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(3906944)))];
            tensor<fp32, [128]> electra_embeddings_LayerNorm_bias = const()[name = tensor<string, []>("electra_embeddings_LayerNorm_bias"), val = tensor<fp32, [128]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(4059712)))];
            tensor<fp32, [128]> electra_embeddings_LayerNorm_weight = const()[name = tensor<string, []>("electra_embeddings_LayerNorm_weight"), val = tensor<fp32, [128]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(4060288)))];
            tensor<fp32, [256]> electra_embeddings_project_bias = const()[name = tensor<string, []>("electra_embeddings_project_bias"), val = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(4060864)))];
            tensor<fp32, [256, 128]> electra_embeddings_project_weight_affine_quantized = constexpr_affine_dequantize()[axis = tensor<int32, []>(0), name = tensor<string, []>("electra_embeddings_project_weight_affine_quantized"), quantized_data = tensor<int8, [256, 128]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(4061952))), scale = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(4095104))), zero_point = tensor<int8, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(4094784)))];
            tensor<fp32, [256]> electra_encoder_layer_0_attention_self_query_bias = const()[name = tensor<string, []>("electra_encoder_layer_0_attention_self_query_bias"), val = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(4096192)))];
            tensor<fp32, [256, 256]> electra_encoder_layer_0_attention_self_query_weight_affine_quantized = constexpr_affine_dequantize()[axis = tensor<int32, []>(0), name = tensor<string, []>("electra_encoder_layer_0_attention_self_query_weight_affine_quantized"), quantized_data = tensor<int8, [256, 256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(4097280))), scale = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(4163200))), zero_point = tensor<int8, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(4162880)))];
            tensor<fp32, [256]> electra_encoder_layer_0_attention_self_key_bias = const()[name = tensor<string, []>("electra_encoder_layer_0_attention_self_key_bias"), val = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(4164288)))];
            tensor<fp32, [256, 256]> electra_encoder_layer_0_attention_self_key_weight_affine_quantized = constexpr_affine_dequantize()[axis = tensor<int32, []>(0), name = tensor<string, []>("electra_encoder_layer_0_attention_self_key_weight_affine_quantized"), quantized_data = tensor<int8, [256, 256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(4165376))), scale = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(4231296))), zero_point = tensor<int8, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(4230976)))];
            tensor<fp32, [256]> electra_encoder_layer_0_attention_self_value_bias = const()[name = tensor<string, []>("electra_encoder_layer_0_attention_self_value_bias"), val = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(4232384)))];
            tensor<fp32, [256, 256]> electra_encoder_layer_0_attention_self_value_weight_affine_quantized = constexpr_affine_dequantize()[axis = tensor<int32, []>(0), name = tensor<string, []>("electra_encoder_layer_0_attention_self_value_weight_affine_quantized"), quantized_data = tensor<int8, [256, 256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(4233472))), scale = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(4299392))), zero_point = tensor<int8, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(4299072)))];
            tensor<fp32, [256]> electra_encoder_layer_0_attention_output_dense_bias = const()[name = tensor<string, []>("electra_encoder_layer_0_attention_output_dense_bias"), val = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(4300480)))];
            tensor<fp32, [256, 256]> electra_encoder_layer_0_attention_output_dense_weight_affine_quantized = constexpr_affine_dequantize()[axis = tensor<int32, []>(0), name = tensor<string, []>("electra_encoder_layer_0_attention_output_dense_weight_affine_quantized"), quantized_data = tensor<int8, [256, 256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(4301568))), scale = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(4367488))), zero_point = tensor<int8, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(4367168)))];
            tensor<fp32, [256]> electra_encoder_layer_0_attention_output_LayerNorm_bias = const()[name = tensor<string, []>("electra_encoder_layer_0_attention_output_LayerNorm_bias"), val = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(4368576)))];
            tensor<fp32, [256]> electra_encoder_layer_0_attention_output_LayerNorm_weight = const()[name = tensor<string, []>("electra_encoder_layer_0_attention_output_LayerNorm_weight"), val = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(4369664)))];
            tensor<fp32, [1024]> electra_encoder_layer_0_intermediate_dense_bias = const()[name = tensor<string, []>("electra_encoder_layer_0_intermediate_dense_bias"), val = tensor<fp32, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(4370752)))];
            tensor<fp32, [1024, 256]> electra_encoder_layer_0_intermediate_dense_weight_affine_quantized = constexpr_affine_dequantize()[axis = tensor<int32, []>(0), name = tensor<string, []>("electra_encoder_layer_0_intermediate_dense_weight_affine_quantized"), quantized_data = tensor<int8, [1024, 256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(4374912))), scale = tensor<fp32, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(4638208))), zero_point = tensor<int8, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(4637120)))];
            tensor<fp32, [256]> electra_encoder_layer_0_output_dense_bias = const()[name = tensor<string, []>("electra_encoder_layer_0_output_dense_bias"), val = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(4642368)))];
            tensor<fp32, [256, 1024]> electra_encoder_layer_0_output_dense_weight_affine_quantized = constexpr_affine_dequantize()[axis = tensor<int32, []>(0), name = tensor<string, []>("electra_encoder_layer_0_output_dense_weight_affine_quantized"), quantized_data = tensor<int8, [256, 1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(4643456))), scale = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(4905984))), zero_point = tensor<int8, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(4905664)))];
            tensor<fp32, [256]> electra_encoder_layer_0_output_LayerNorm_bias = const()[name = tensor<string, []>("electra_encoder_layer_0_output_LayerNorm_bias"), val = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(4907072)))];
            tensor<fp32, [256]> electra_encoder_layer_0_output_LayerNorm_weight = const()[name = tensor<string, []>("electra_encoder_layer_0_output_LayerNorm_weight"), val = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(4908160)))];
            tensor<fp32, [256]> electra_encoder_layer_1_attention_self_query_bias = const()[name = tensor<string, []>("electra_encoder_layer_1_attention_self_query_bias"), val = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(4909248)))];
            tensor<fp32, [256, 256]> electra_encoder_layer_1_attention_self_query_weight_affine_quantized = constexpr_affine_dequantize()[axis = tensor<int32, []>(0), name = tensor<string, []>("electra_encoder_layer_1_attention_self_query_weight_affine_quantized"), quantized_data = tensor<int8, [256, 256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(4910336))), scale = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(4976256))), zero_point = tensor<int8, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(4975936)))];
            tensor<fp32, [256]> electra_encoder_layer_1_attention_self_key_bias = const()[name = tensor<string, []>("electra_encoder_layer_1_attention_self_key_bias"), val = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(4977344)))];
            tensor<fp32, [256, 256]> electra_encoder_layer_1_attention_self_key_weight_affine_quantized = constexpr_affine_dequantize()[axis = tensor<int32, []>(0), name = tensor<string, []>("electra_encoder_layer_1_attention_self_key_weight_affine_quantized"), quantized_data = tensor<int8, [256, 256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(4978432))), scale = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(5044352))), zero_point = tensor<int8, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(5044032)))];
            tensor<fp32, [256]> electra_encoder_layer_1_attention_self_value_bias = const()[name = tensor<string, []>("electra_encoder_layer_1_attention_self_value_bias"), val = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(5045440)))];
            tensor<fp32, [256, 256]> electra_encoder_layer_1_attention_self_value_weight_affine_quantized = constexpr_affine_dequantize()[axis = tensor<int32, []>(0), name = tensor<string, []>("electra_encoder_layer_1_attention_self_value_weight_affine_quantized"), quantized_data = tensor<int8, [256, 256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(5046528))), scale = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(5112448))), zero_point = tensor<int8, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(5112128)))];
            tensor<fp32, [256]> electra_encoder_layer_1_attention_output_dense_bias = const()[name = tensor<string, []>("electra_encoder_layer_1_attention_output_dense_bias"), val = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(5113536)))];
            tensor<fp32, [256, 256]> electra_encoder_layer_1_attention_output_dense_weight_affine_quantized = constexpr_affine_dequantize()[axis = tensor<int32, []>(0), name = tensor<string, []>("electra_encoder_layer_1_attention_output_dense_weight_affine_quantized"), quantized_data = tensor<int8, [256, 256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(5114624))), scale = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(5180544))), zero_point = tensor<int8, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(5180224)))];
            tensor<fp32, [256]> electra_encoder_layer_1_attention_output_LayerNorm_bias = const()[name = tensor<string, []>("electra_encoder_layer_1_attention_output_LayerNorm_bias"), val = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(5181632)))];
            tensor<fp32, [256]> electra_encoder_layer_1_attention_output_LayerNorm_weight = const()[name = tensor<string, []>("electra_encoder_layer_1_attention_output_LayerNorm_weight"), val = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(5182720)))];
            tensor<fp32, [1024]> electra_encoder_layer_1_intermediate_dense_bias = const()[name = tensor<string, []>("electra_encoder_layer_1_intermediate_dense_bias"), val = tensor<fp32, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(5183808)))];
            tensor<fp32, [1024, 256]> electra_encoder_layer_1_intermediate_dense_weight_affine_quantized = constexpr_affine_dequantize()[axis = tensor<int32, []>(0), name = tensor<string, []>("electra_encoder_layer_1_intermediate_dense_weight_affine_quantized"), quantized_data = tensor<int8, [1024, 256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(5187968))), scale = tensor<fp32, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(5451264))), zero_point = tensor<int8, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(5450176)))];
            tensor<fp32, [256]> electra_encoder_layer_1_output_dense_bias = const()[name = tensor<string, []>("electra_encoder_layer_1_output_dense_bias"), val = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(5455424)))];
            tensor<fp32, [256, 1024]> electra_encoder_layer_1_output_dense_weight_affine_quantized = constexpr_affine_dequantize()[axis = tensor<int32, []>(0), name = tensor<string, []>("electra_encoder_layer_1_output_dense_weight_affine_quantized"), quantized_data = tensor<int8, [256, 1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(5456512))), scale = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(5719040))), zero_point = tensor<int8, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(5718720)))];
            tensor<fp32, [256]> electra_encoder_layer_1_output_LayerNorm_bias = const()[name = tensor<string, []>("electra_encoder_layer_1_output_LayerNorm_bias"), val = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(5720128)))];
            tensor<fp32, [256]> electra_encoder_layer_1_output_LayerNorm_weight = const()[name = tensor<string, []>("electra_encoder_layer_1_output_LayerNorm_weight"), val = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(5721216)))];
            tensor<fp32, [256]> electra_encoder_layer_2_attention_self_query_bias = const()[name = tensor<string, []>("electra_encoder_layer_2_attention_self_query_bias"), val = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(5722304)))];
            tensor<fp32, [256, 256]> electra_encoder_layer_2_attention_self_query_weight_affine_quantized = constexpr_affine_dequantize()[axis = tensor<int32, []>(0), name = tensor<string, []>("electra_encoder_layer_2_attention_self_query_weight_affine_quantized"), quantized_data = tensor<int8, [256, 256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(5723392))), scale = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(5789312))), zero_point = tensor<int8, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(5788992)))];
            tensor<fp32, [256]> electra_encoder_layer_2_attention_self_key_bias = const()[name = tensor<string, []>("electra_encoder_layer_2_attention_self_key_bias"), val = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(5790400)))];
            tensor<fp32, [256, 256]> electra_encoder_layer_2_attention_self_key_weight_affine_quantized = constexpr_affine_dequantize()[axis = tensor<int32, []>(0), name = tensor<string, []>("electra_encoder_layer_2_attention_self_key_weight_affine_quantized"), quantized_data = tensor<int8, [256, 256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(5791488))), scale = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(5857408))), zero_point = tensor<int8, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(5857088)))];
            tensor<fp32, [256]> electra_encoder_layer_2_attention_self_value_bias = const()[name = tensor<string, []>("electra_encoder_layer_2_attention_self_value_bias"), val = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(5858496)))];
            tensor<fp32, [256, 256]> electra_encoder_layer_2_attention_self_value_weight_affine_quantized = constexpr_affine_dequantize()[axis = tensor<int32, []>(0), name = tensor<string, []>("electra_encoder_layer_2_attention_self_value_weight_affine_quantized"), quantized_data = tensor<int8, [256, 256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(5859584))), scale = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(5925504))), zero_point = tensor<int8, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(5925184)))];
            tensor<fp32, [256]> electra_encoder_layer_2_attention_output_dense_bias = const()[name = tensor<string, []>("electra_encoder_layer_2_attention_output_dense_bias"), val = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(5926592)))];
            tensor<fp32, [256, 256]> electra_encoder_layer_2_attention_output_dense_weight_affine_quantized = constexpr_affine_dequantize()[axis = tensor<int32, []>(0), name = tensor<string, []>("electra_encoder_layer_2_attention_output_dense_weight_affine_quantized"), quantized_data = tensor<int8, [256, 256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(5927680))), scale = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(5993600))), zero_point = tensor<int8, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(5993280)))];
            tensor<fp32, [256]> electra_encoder_layer_2_attention_output_LayerNorm_bias = const()[name = tensor<string, []>("electra_encoder_layer_2_attention_output_LayerNorm_bias"), val = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(5994688)))];
            tensor<fp32, [256]> electra_encoder_layer_2_attention_output_LayerNorm_weight = const()[name = tensor<string, []>("electra_encoder_layer_2_attention_output_LayerNorm_weight"), val = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(5995776)))];
            tensor<fp32, [1024]> electra_encoder_layer_2_intermediate_dense_bias = const()[name = tensor<string, []>("electra_encoder_layer_2_intermediate_dense_bias"), val = tensor<fp32, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(5996864)))];
            tensor<fp32, [1024, 256]> electra_encoder_layer_2_intermediate_dense_weight_affine_quantized = constexpr_affine_dequantize()[axis = tensor<int32, []>(0), name = tensor<string, []>("electra_encoder_layer_2_intermediate_dense_weight_affine_quantized"), quantized_data = tensor<int8, [1024, 256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(6001024))), scale = tensor<fp32, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(6264320))), zero_point = tensor<int8, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(6263232)))];
            tensor<fp32, [256]> electra_encoder_layer_2_output_dense_bias = const()[name = tensor<string, []>("electra_encoder_layer_2_output_dense_bias"), val = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(6268480)))];
            tensor<fp32, [256, 1024]> electra_encoder_layer_2_output_dense_weight_affine_quantized = constexpr_affine_dequantize()[axis = tensor<int32, []>(0), name = tensor<string, []>("electra_encoder_layer_2_output_dense_weight_affine_quantized"), quantized_data = tensor<int8, [256, 1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(6269568))), scale = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(6532096))), zero_point = tensor<int8, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(6531776)))];
            tensor<fp32, [256]> electra_encoder_layer_2_output_LayerNorm_bias = const()[name = tensor<string, []>("electra_encoder_layer_2_output_LayerNorm_bias"), val = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(6533184)))];
            tensor<fp32, [256]> electra_encoder_layer_2_output_LayerNorm_weight = const()[name = tensor<string, []>("electra_encoder_layer_2_output_LayerNorm_weight"), val = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(6534272)))];
            tensor<fp32, [256]> electra_encoder_layer_3_attention_self_query_bias = const()[name = tensor<string, []>("electra_encoder_layer_3_attention_self_query_bias"), val = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(6535360)))];
            tensor<fp32, [256, 256]> electra_encoder_layer_3_attention_self_query_weight_affine_quantized = constexpr_affine_dequantize()[axis = tensor<int32, []>(0), name = tensor<string, []>("electra_encoder_layer_3_attention_self_query_weight_affine_quantized"), quantized_data = tensor<int8, [256, 256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(6536448))), scale = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(6602368))), zero_point = tensor<int8, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(6602048)))];
            tensor<fp32, [256]> electra_encoder_layer_3_attention_self_key_bias = const()[name = tensor<string, []>("electra_encoder_layer_3_attention_self_key_bias"), val = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(6603456)))];
            tensor<fp32, [256, 256]> electra_encoder_layer_3_attention_self_key_weight_affine_quantized = constexpr_affine_dequantize()[axis = tensor<int32, []>(0), name = tensor<string, []>("electra_encoder_layer_3_attention_self_key_weight_affine_quantized"), quantized_data = tensor<int8, [256, 256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(6604544))), scale = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(6670464))), zero_point = tensor<int8, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(6670144)))];
            tensor<fp32, [256]> electra_encoder_layer_3_attention_self_value_bias = const()[name = tensor<string, []>("electra_encoder_layer_3_attention_self_value_bias"), val = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(6671552)))];
            tensor<fp32, [256, 256]> electra_encoder_layer_3_attention_self_value_weight_affine_quantized = constexpr_affine_dequantize()[axis = tensor<int32, []>(0), name = tensor<string, []>("electra_encoder_layer_3_attention_self_value_weight_affine_quantized"), quantized_data = tensor<int8, [256, 256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(6672640))), scale = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(6738560))), zero_point = tensor<int8, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(6738240)))];
            tensor<fp32, [256]> electra_encoder_layer_3_attention_output_dense_bias = const()[name = tensor<string, []>("electra_encoder_layer_3_attention_output_dense_bias"), val = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(6739648)))];
            tensor<fp32, [256, 256]> electra_encoder_layer_3_attention_output_dense_weight_affine_quantized = constexpr_affine_dequantize()[axis = tensor<int32, []>(0), name = tensor<string, []>("electra_encoder_layer_3_attention_output_dense_weight_affine_quantized"), quantized_data = tensor<int8, [256, 256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(6740736))), scale = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(6806656))), zero_point = tensor<int8, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(6806336)))];
            tensor<fp32, [256]> electra_encoder_layer_3_attention_output_LayerNorm_bias = const()[name = tensor<string, []>("electra_encoder_layer_3_attention_output_LayerNorm_bias"), val = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(6807744)))];
            tensor<fp32, [256]> electra_encoder_layer_3_attention_output_LayerNorm_weight = const()[name = tensor<string, []>("electra_encoder_layer_3_attention_output_LayerNorm_weight"), val = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(6808832)))];
            tensor<fp32, [1024]> electra_encoder_layer_3_intermediate_dense_bias = const()[name = tensor<string, []>("electra_encoder_layer_3_intermediate_dense_bias"), val = tensor<fp32, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(6809920)))];
            tensor<fp32, [1024, 256]> electra_encoder_layer_3_intermediate_dense_weight_affine_quantized = constexpr_affine_dequantize()[axis = tensor<int32, []>(0), name = tensor<string, []>("electra_encoder_layer_3_intermediate_dense_weight_affine_quantized"), quantized_data = tensor<int8, [1024, 256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(6814080))), scale = tensor<fp32, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(7077376))), zero_point = tensor<int8, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(7076288)))];
            tensor<fp32, [256]> electra_encoder_layer_3_output_dense_bias = const()[name = tensor<string, []>("electra_encoder_layer_3_output_dense_bias"), val = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(7081536)))];
            tensor<fp32, [256, 1024]> electra_encoder_layer_3_output_dense_weight_affine_quantized = constexpr_affine_dequantize()[axis = tensor<int32, []>(0), name = tensor<string, []>("electra_encoder_layer_3_output_dense_weight_affine_quantized"), quantized_data = tensor<int8, [256, 1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(7082624))), scale = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(7345152))), zero_point = tensor<int8, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(7344832)))];
            tensor<fp32, [256]> electra_encoder_layer_3_output_LayerNorm_bias = const()[name = tensor<string, []>("electra_encoder_layer_3_output_LayerNorm_bias"), val = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(7346240)))];
            tensor<fp32, [256]> electra_encoder_layer_3_output_LayerNorm_weight = const()[name = tensor<string, []>("electra_encoder_layer_3_output_LayerNorm_weight"), val = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(7347328)))];
            tensor<fp32, [256]> electra_encoder_layer_4_attention_self_query_bias = const()[name = tensor<string, []>("electra_encoder_layer_4_attention_self_query_bias"), val = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(7348416)))];
            tensor<fp32, [256, 256]> electra_encoder_layer_4_attention_self_query_weight_affine_quantized = constexpr_affine_dequantize()[axis = tensor<int32, []>(0), name = tensor<string, []>("electra_encoder_layer_4_attention_self_query_weight_affine_quantized"), quantized_data = tensor<int8, [256, 256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(7349504))), scale = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(7415424))), zero_point = tensor<int8, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(7415104)))];
            tensor<fp32, [256]> electra_encoder_layer_4_attention_self_key_bias = const()[name = tensor<string, []>("electra_encoder_layer_4_attention_self_key_bias"), val = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(7416512)))];
            tensor<fp32, [256, 256]> electra_encoder_layer_4_attention_self_key_weight_affine_quantized = constexpr_affine_dequantize()[axis = tensor<int32, []>(0), name = tensor<string, []>("electra_encoder_layer_4_attention_self_key_weight_affine_quantized"), quantized_data = tensor<int8, [256, 256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(7417600))), scale = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(7483520))), zero_point = tensor<int8, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(7483200)))];
            tensor<fp32, [256]> electra_encoder_layer_4_attention_self_value_bias = const()[name = tensor<string, []>("electra_encoder_layer_4_attention_self_value_bias"), val = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(7484608)))];
            tensor<fp32, [256, 256]> electra_encoder_layer_4_attention_self_value_weight_affine_quantized = constexpr_affine_dequantize()[axis = tensor<int32, []>(0), name = tensor<string, []>("electra_encoder_layer_4_attention_self_value_weight_affine_quantized"), quantized_data = tensor<int8, [256, 256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(7485696))), scale = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(7551616))), zero_point = tensor<int8, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(7551296)))];
            tensor<fp32, [256]> electra_encoder_layer_4_attention_output_dense_bias = const()[name = tensor<string, []>("electra_encoder_layer_4_attention_output_dense_bias"), val = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(7552704)))];
            tensor<fp32, [256, 256]> electra_encoder_layer_4_attention_output_dense_weight_affine_quantized = constexpr_affine_dequantize()[axis = tensor<int32, []>(0), name = tensor<string, []>("electra_encoder_layer_4_attention_output_dense_weight_affine_quantized"), quantized_data = tensor<int8, [256, 256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(7553792))), scale = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(7619712))), zero_point = tensor<int8, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(7619392)))];
            tensor<fp32, [256]> electra_encoder_layer_4_attention_output_LayerNorm_bias = const()[name = tensor<string, []>("electra_encoder_layer_4_attention_output_LayerNorm_bias"), val = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(7620800)))];
            tensor<fp32, [256]> electra_encoder_layer_4_attention_output_LayerNorm_weight = const()[name = tensor<string, []>("electra_encoder_layer_4_attention_output_LayerNorm_weight"), val = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(7621888)))];
            tensor<fp32, [1024]> electra_encoder_layer_4_intermediate_dense_bias = const()[name = tensor<string, []>("electra_encoder_layer_4_intermediate_dense_bias"), val = tensor<fp32, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(7622976)))];
            tensor<fp32, [1024, 256]> electra_encoder_layer_4_intermediate_dense_weight_affine_quantized = constexpr_affine_dequantize()[axis = tensor<int32, []>(0), name = tensor<string, []>("electra_encoder_layer_4_intermediate_dense_weight_affine_quantized"), quantized_data = tensor<int8, [1024, 256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(7627136))), scale = tensor<fp32, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(7890432))), zero_point = tensor<int8, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(7889344)))];
            tensor<fp32, [256]> electra_encoder_layer_4_output_dense_bias = const()[name = tensor<string, []>("electra_encoder_layer_4_output_dense_bias"), val = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(7894592)))];
            tensor<fp32, [256, 1024]> electra_encoder_layer_4_output_dense_weight_affine_quantized = constexpr_affine_dequantize()[axis = tensor<int32, []>(0), name = tensor<string, []>("electra_encoder_layer_4_output_dense_weight_affine_quantized"), quantized_data = tensor<int8, [256, 1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(7895680))), scale = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(8158208))), zero_point = tensor<int8, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(8157888)))];
            tensor<fp32, [256]> electra_encoder_layer_4_output_LayerNorm_bias = const()[name = tensor<string, []>("electra_encoder_layer_4_output_LayerNorm_bias"), val = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(8159296)))];
            tensor<fp32, [256]> electra_encoder_layer_4_output_LayerNorm_weight = const()[name = tensor<string, []>("electra_encoder_layer_4_output_LayerNorm_weight"), val = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(8160384)))];
            tensor<fp32, [256]> electra_encoder_layer_5_attention_self_query_bias = const()[name = tensor<string, []>("electra_encoder_layer_5_attention_self_query_bias"), val = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(8161472)))];
            tensor<fp32, [256, 256]> electra_encoder_layer_5_attention_self_query_weight_affine_quantized = constexpr_affine_dequantize()[axis = tensor<int32, []>(0), name = tensor<string, []>("electra_encoder_layer_5_attention_self_query_weight_affine_quantized"), quantized_data = tensor<int8, [256, 256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(8162560))), scale = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(8228480))), zero_point = tensor<int8, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(8228160)))];
            tensor<fp32, [256]> electra_encoder_layer_5_attention_self_key_bias = const()[name = tensor<string, []>("electra_encoder_layer_5_attention_self_key_bias"), val = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(8229568)))];
            tensor<fp32, [256, 256]> electra_encoder_layer_5_attention_self_key_weight_affine_quantized = constexpr_affine_dequantize()[axis = tensor<int32, []>(0), name = tensor<string, []>("electra_encoder_layer_5_attention_self_key_weight_affine_quantized"), quantized_data = tensor<int8, [256, 256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(8230656))), scale = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(8296576))), zero_point = tensor<int8, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(8296256)))];
            tensor<fp32, [256]> electra_encoder_layer_5_attention_self_value_bias = const()[name = tensor<string, []>("electra_encoder_layer_5_attention_self_value_bias"), val = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(8297664)))];
            tensor<fp32, [256, 256]> electra_encoder_layer_5_attention_self_value_weight_affine_quantized = constexpr_affine_dequantize()[axis = tensor<int32, []>(0), name = tensor<string, []>("electra_encoder_layer_5_attention_self_value_weight_affine_quantized"), quantized_data = tensor<int8, [256, 256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(8298752))), scale = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(8364672))), zero_point = tensor<int8, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(8364352)))];
            tensor<fp32, [256]> electra_encoder_layer_5_attention_output_dense_bias = const()[name = tensor<string, []>("electra_encoder_layer_5_attention_output_dense_bias"), val = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(8365760)))];
            tensor<fp32, [256, 256]> electra_encoder_layer_5_attention_output_dense_weight_affine_quantized = constexpr_affine_dequantize()[axis = tensor<int32, []>(0), name = tensor<string, []>("electra_encoder_layer_5_attention_output_dense_weight_affine_quantized"), quantized_data = tensor<int8, [256, 256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(8366848))), scale = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(8432768))), zero_point = tensor<int8, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(8432448)))];
            tensor<fp32, [256]> electra_encoder_layer_5_attention_output_LayerNorm_bias = const()[name = tensor<string, []>("electra_encoder_layer_5_attention_output_LayerNorm_bias"), val = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(8433856)))];
            tensor<fp32, [256]> electra_encoder_layer_5_attention_output_LayerNorm_weight = const()[name = tensor<string, []>("electra_encoder_layer_5_attention_output_LayerNorm_weight"), val = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(8434944)))];
            tensor<fp32, [1024]> electra_encoder_layer_5_intermediate_dense_bias = const()[name = tensor<string, []>("electra_encoder_layer_5_intermediate_dense_bias"), val = tensor<fp32, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(8436032)))];
            tensor<fp32, [1024, 256]> electra_encoder_layer_5_intermediate_dense_weight_affine_quantized = constexpr_affine_dequantize()[axis = tensor<int32, []>(0), name = tensor<string, []>("electra_encoder_layer_5_intermediate_dense_weight_affine_quantized"), quantized_data = tensor<int8, [1024, 256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(8440192))), scale = tensor<fp32, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(8703488))), zero_point = tensor<int8, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(8702400)))];
            tensor<fp32, [256]> electra_encoder_layer_5_output_dense_bias = const()[name = tensor<string, []>("electra_encoder_layer_5_output_dense_bias"), val = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(8707648)))];
            tensor<fp32, [256, 1024]> electra_encoder_layer_5_output_dense_weight_affine_quantized = constexpr_affine_dequantize()[axis = tensor<int32, []>(0), name = tensor<string, []>("electra_encoder_layer_5_output_dense_weight_affine_quantized"), quantized_data = tensor<int8, [256, 1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(8708736))), scale = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(8971264))), zero_point = tensor<int8, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(8970944)))];
            tensor<fp32, [256]> electra_encoder_layer_5_output_LayerNorm_bias = const()[name = tensor<string, []>("electra_encoder_layer_5_output_LayerNorm_bias"), val = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(8972352)))];
            tensor<fp32, [256]> electra_encoder_layer_5_output_LayerNorm_weight = const()[name = tensor<string, []>("electra_encoder_layer_5_output_LayerNorm_weight"), val = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(8973440)))];
            tensor<fp32, [256]> electra_encoder_layer_6_attention_self_query_bias = const()[name = tensor<string, []>("electra_encoder_layer_6_attention_self_query_bias"), val = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(8974528)))];
            tensor<fp32, [256, 256]> electra_encoder_layer_6_attention_self_query_weight_affine_quantized = constexpr_affine_dequantize()[axis = tensor<int32, []>(0), name = tensor<string, []>("electra_encoder_layer_6_attention_self_query_weight_affine_quantized"), quantized_data = tensor<int8, [256, 256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(8975616))), scale = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(9041536))), zero_point = tensor<int8, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(9041216)))];
            tensor<fp32, [256]> electra_encoder_layer_6_attention_self_key_bias = const()[name = tensor<string, []>("electra_encoder_layer_6_attention_self_key_bias"), val = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(9042624)))];
            tensor<fp32, [256, 256]> electra_encoder_layer_6_attention_self_key_weight_affine_quantized = constexpr_affine_dequantize()[axis = tensor<int32, []>(0), name = tensor<string, []>("electra_encoder_layer_6_attention_self_key_weight_affine_quantized"), quantized_data = tensor<int8, [256, 256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(9043712))), scale = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(9109632))), zero_point = tensor<int8, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(9109312)))];
            tensor<fp32, [256]> electra_encoder_layer_6_attention_self_value_bias = const()[name = tensor<string, []>("electra_encoder_layer_6_attention_self_value_bias"), val = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(9110720)))];
            tensor<fp32, [256, 256]> electra_encoder_layer_6_attention_self_value_weight_affine_quantized = constexpr_affine_dequantize()[axis = tensor<int32, []>(0), name = tensor<string, []>("electra_encoder_layer_6_attention_self_value_weight_affine_quantized"), quantized_data = tensor<int8, [256, 256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(9111808))), scale = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(9177728))), zero_point = tensor<int8, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(9177408)))];
            tensor<fp32, [256]> electra_encoder_layer_6_attention_output_dense_bias = const()[name = tensor<string, []>("electra_encoder_layer_6_attention_output_dense_bias"), val = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(9178816)))];
            tensor<fp32, [256, 256]> electra_encoder_layer_6_attention_output_dense_weight_affine_quantized = constexpr_affine_dequantize()[axis = tensor<int32, []>(0), name = tensor<string, []>("electra_encoder_layer_6_attention_output_dense_weight_affine_quantized"), quantized_data = tensor<int8, [256, 256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(9179904))), scale = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(9245824))), zero_point = tensor<int8, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(9245504)))];
            tensor<fp32, [256]> electra_encoder_layer_6_attention_output_LayerNorm_bias = const()[name = tensor<string, []>("electra_encoder_layer_6_attention_output_LayerNorm_bias"), val = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(9246912)))];
            tensor<fp32, [256]> electra_encoder_layer_6_attention_output_LayerNorm_weight = const()[name = tensor<string, []>("electra_encoder_layer_6_attention_output_LayerNorm_weight"), val = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(9248000)))];
            tensor<fp32, [1024]> electra_encoder_layer_6_intermediate_dense_bias = const()[name = tensor<string, []>("electra_encoder_layer_6_intermediate_dense_bias"), val = tensor<fp32, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(9249088)))];
            tensor<fp32, [1024, 256]> electra_encoder_layer_6_intermediate_dense_weight_affine_quantized = constexpr_affine_dequantize()[axis = tensor<int32, []>(0), name = tensor<string, []>("electra_encoder_layer_6_intermediate_dense_weight_affine_quantized"), quantized_data = tensor<int8, [1024, 256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(9253248))), scale = tensor<fp32, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(9516544))), zero_point = tensor<int8, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(9515456)))];
            tensor<fp32, [256]> electra_encoder_layer_6_output_dense_bias = const()[name = tensor<string, []>("electra_encoder_layer_6_output_dense_bias"), val = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(9520704)))];
            tensor<fp32, [256, 1024]> electra_encoder_layer_6_output_dense_weight_affine_quantized = constexpr_affine_dequantize()[axis = tensor<int32, []>(0), name = tensor<string, []>("electra_encoder_layer_6_output_dense_weight_affine_quantized"), quantized_data = tensor<int8, [256, 1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(9521792))), scale = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(9784320))), zero_point = tensor<int8, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(9784000)))];
            tensor<fp32, [256]> electra_encoder_layer_6_output_LayerNorm_bias = const()[name = tensor<string, []>("electra_encoder_layer_6_output_LayerNorm_bias"), val = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(9785408)))];
            tensor<fp32, [256]> electra_encoder_layer_6_output_LayerNorm_weight = const()[name = tensor<string, []>("electra_encoder_layer_6_output_LayerNorm_weight"), val = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(9786496)))];
            tensor<fp32, [256]> electra_encoder_layer_7_attention_self_query_bias = const()[name = tensor<string, []>("electra_encoder_layer_7_attention_self_query_bias"), val = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(9787584)))];
            tensor<fp32, [256, 256]> electra_encoder_layer_7_attention_self_query_weight_affine_quantized = constexpr_affine_dequantize()[axis = tensor<int32, []>(0), name = tensor<string, []>("electra_encoder_layer_7_attention_self_query_weight_affine_quantized"), quantized_data = tensor<int8, [256, 256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(9788672))), scale = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(9854592))), zero_point = tensor<int8, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(9854272)))];
            tensor<fp32, [256]> electra_encoder_layer_7_attention_self_key_bias = const()[name = tensor<string, []>("electra_encoder_layer_7_attention_self_key_bias"), val = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(9855680)))];
            tensor<fp32, [256, 256]> electra_encoder_layer_7_attention_self_key_weight_affine_quantized = constexpr_affine_dequantize()[axis = tensor<int32, []>(0), name = tensor<string, []>("electra_encoder_layer_7_attention_self_key_weight_affine_quantized"), quantized_data = tensor<int8, [256, 256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(9856768))), scale = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(9922688))), zero_point = tensor<int8, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(9922368)))];
            tensor<fp32, [256]> electra_encoder_layer_7_attention_self_value_bias = const()[name = tensor<string, []>("electra_encoder_layer_7_attention_self_value_bias"), val = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(9923776)))];
            tensor<fp32, [256, 256]> electra_encoder_layer_7_attention_self_value_weight_affine_quantized = constexpr_affine_dequantize()[axis = tensor<int32, []>(0), name = tensor<string, []>("electra_encoder_layer_7_attention_self_value_weight_affine_quantized"), quantized_data = tensor<int8, [256, 256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(9924864))), scale = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(9990784))), zero_point = tensor<int8, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(9990464)))];
            tensor<fp32, [256]> electra_encoder_layer_7_attention_output_dense_bias = const()[name = tensor<string, []>("electra_encoder_layer_7_attention_output_dense_bias"), val = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(9991872)))];
            tensor<fp32, [256, 256]> electra_encoder_layer_7_attention_output_dense_weight_affine_quantized = constexpr_affine_dequantize()[axis = tensor<int32, []>(0), name = tensor<string, []>("electra_encoder_layer_7_attention_output_dense_weight_affine_quantized"), quantized_data = tensor<int8, [256, 256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(9992960))), scale = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(10058880))), zero_point = tensor<int8, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(10058560)))];
            tensor<fp32, [256]> electra_encoder_layer_7_attention_output_LayerNorm_bias = const()[name = tensor<string, []>("electra_encoder_layer_7_attention_output_LayerNorm_bias"), val = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(10059968)))];
            tensor<fp32, [256]> electra_encoder_layer_7_attention_output_LayerNorm_weight = const()[name = tensor<string, []>("electra_encoder_layer_7_attention_output_LayerNorm_weight"), val = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(10061056)))];
            tensor<fp32, [1024]> electra_encoder_layer_7_intermediate_dense_bias = const()[name = tensor<string, []>("electra_encoder_layer_7_intermediate_dense_bias"), val = tensor<fp32, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(10062144)))];
            tensor<fp32, [1024, 256]> electra_encoder_layer_7_intermediate_dense_weight_affine_quantized = constexpr_affine_dequantize()[axis = tensor<int32, []>(0), name = tensor<string, []>("electra_encoder_layer_7_intermediate_dense_weight_affine_quantized"), quantized_data = tensor<int8, [1024, 256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(10066304))), scale = tensor<fp32, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(10329600))), zero_point = tensor<int8, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(10328512)))];
            tensor<fp32, [256]> electra_encoder_layer_7_output_dense_bias = const()[name = tensor<string, []>("electra_encoder_layer_7_output_dense_bias"), val = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(10333760)))];
            tensor<fp32, [256, 1024]> electra_encoder_layer_7_output_dense_weight_affine_quantized = constexpr_affine_dequantize()[axis = tensor<int32, []>(0), name = tensor<string, []>("electra_encoder_layer_7_output_dense_weight_affine_quantized"), quantized_data = tensor<int8, [256, 1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(10334848))), scale = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(10597376))), zero_point = tensor<int8, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(10597056)))];
            tensor<fp32, [256]> electra_encoder_layer_7_output_LayerNorm_bias = const()[name = tensor<string, []>("electra_encoder_layer_7_output_LayerNorm_bias"), val = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(10598464)))];
            tensor<fp32, [256]> electra_encoder_layer_7_output_LayerNorm_weight = const()[name = tensor<string, []>("electra_encoder_layer_7_output_LayerNorm_weight"), val = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(10599552)))];
            tensor<fp32, [256]> electra_encoder_layer_8_attention_self_query_bias = const()[name = tensor<string, []>("electra_encoder_layer_8_attention_self_query_bias"), val = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(10600640)))];
            tensor<fp32, [256, 256]> electra_encoder_layer_8_attention_self_query_weight_affine_quantized = constexpr_affine_dequantize()[axis = tensor<int32, []>(0), name = tensor<string, []>("electra_encoder_layer_8_attention_self_query_weight_affine_quantized"), quantized_data = tensor<int8, [256, 256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(10601728))), scale = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(10667648))), zero_point = tensor<int8, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(10667328)))];
            tensor<fp32, [256]> electra_encoder_layer_8_attention_self_key_bias = const()[name = tensor<string, []>("electra_encoder_layer_8_attention_self_key_bias"), val = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(10668736)))];
            tensor<fp32, [256, 256]> electra_encoder_layer_8_attention_self_key_weight_affine_quantized = constexpr_affine_dequantize()[axis = tensor<int32, []>(0), name = tensor<string, []>("electra_encoder_layer_8_attention_self_key_weight_affine_quantized"), quantized_data = tensor<int8, [256, 256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(10669824))), scale = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(10735744))), zero_point = tensor<int8, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(10735424)))];
            tensor<fp32, [256]> electra_encoder_layer_8_attention_self_value_bias = const()[name = tensor<string, []>("electra_encoder_layer_8_attention_self_value_bias"), val = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(10736832)))];
            tensor<fp32, [256, 256]> electra_encoder_layer_8_attention_self_value_weight_affine_quantized = constexpr_affine_dequantize()[axis = tensor<int32, []>(0), name = tensor<string, []>("electra_encoder_layer_8_attention_self_value_weight_affine_quantized"), quantized_data = tensor<int8, [256, 256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(10737920))), scale = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(10803840))), zero_point = tensor<int8, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(10803520)))];
            tensor<fp32, [256]> electra_encoder_layer_8_attention_output_dense_bias = const()[name = tensor<string, []>("electra_encoder_layer_8_attention_output_dense_bias"), val = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(10804928)))];
            tensor<fp32, [256, 256]> electra_encoder_layer_8_attention_output_dense_weight_affine_quantized = constexpr_affine_dequantize()[axis = tensor<int32, []>(0), name = tensor<string, []>("electra_encoder_layer_8_attention_output_dense_weight_affine_quantized"), quantized_data = tensor<int8, [256, 256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(10806016))), scale = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(10871936))), zero_point = tensor<int8, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(10871616)))];
            tensor<fp32, [256]> electra_encoder_layer_8_attention_output_LayerNorm_bias = const()[name = tensor<string, []>("electra_encoder_layer_8_attention_output_LayerNorm_bias"), val = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(10873024)))];
            tensor<fp32, [256]> electra_encoder_layer_8_attention_output_LayerNorm_weight = const()[name = tensor<string, []>("electra_encoder_layer_8_attention_output_LayerNorm_weight"), val = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(10874112)))];
            tensor<fp32, [1024]> electra_encoder_layer_8_intermediate_dense_bias = const()[name = tensor<string, []>("electra_encoder_layer_8_intermediate_dense_bias"), val = tensor<fp32, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(10875200)))];
            tensor<fp32, [1024, 256]> electra_encoder_layer_8_intermediate_dense_weight_affine_quantized = constexpr_affine_dequantize()[axis = tensor<int32, []>(0), name = tensor<string, []>("electra_encoder_layer_8_intermediate_dense_weight_affine_quantized"), quantized_data = tensor<int8, [1024, 256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(10879360))), scale = tensor<fp32, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(11142656))), zero_point = tensor<int8, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(11141568)))];
            tensor<fp32, [256]> electra_encoder_layer_8_output_dense_bias = const()[name = tensor<string, []>("electra_encoder_layer_8_output_dense_bias"), val = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(11146816)))];
            tensor<fp32, [256, 1024]> electra_encoder_layer_8_output_dense_weight_affine_quantized = constexpr_affine_dequantize()[axis = tensor<int32, []>(0), name = tensor<string, []>("electra_encoder_layer_8_output_dense_weight_affine_quantized"), quantized_data = tensor<int8, [256, 1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(11147904))), scale = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(11410432))), zero_point = tensor<int8, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(11410112)))];
            tensor<fp32, [256]> electra_encoder_layer_8_output_LayerNorm_bias = const()[name = tensor<string, []>("electra_encoder_layer_8_output_LayerNorm_bias"), val = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(11411520)))];
            tensor<fp32, [256]> electra_encoder_layer_8_output_LayerNorm_weight = const()[name = tensor<string, []>("electra_encoder_layer_8_output_LayerNorm_weight"), val = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(11412608)))];
            tensor<fp32, [256]> electra_encoder_layer_9_attention_self_query_bias = const()[name = tensor<string, []>("electra_encoder_layer_9_attention_self_query_bias"), val = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(11413696)))];
            tensor<fp32, [256, 256]> electra_encoder_layer_9_attention_self_query_weight_affine_quantized = constexpr_affine_dequantize()[axis = tensor<int32, []>(0), name = tensor<string, []>("electra_encoder_layer_9_attention_self_query_weight_affine_quantized"), quantized_data = tensor<int8, [256, 256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(11414784))), scale = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(11480704))), zero_point = tensor<int8, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(11480384)))];
            tensor<fp32, [256]> electra_encoder_layer_9_attention_self_key_bias = const()[name = tensor<string, []>("electra_encoder_layer_9_attention_self_key_bias"), val = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(11481792)))];
            tensor<fp32, [256, 256]> electra_encoder_layer_9_attention_self_key_weight_affine_quantized = constexpr_affine_dequantize()[axis = tensor<int32, []>(0), name = tensor<string, []>("electra_encoder_layer_9_attention_self_key_weight_affine_quantized"), quantized_data = tensor<int8, [256, 256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(11482880))), scale = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(11548800))), zero_point = tensor<int8, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(11548480)))];
            tensor<fp32, [256]> electra_encoder_layer_9_attention_self_value_bias = const()[name = tensor<string, []>("electra_encoder_layer_9_attention_self_value_bias"), val = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(11549888)))];
            tensor<fp32, [256, 256]> electra_encoder_layer_9_attention_self_value_weight_affine_quantized = constexpr_affine_dequantize()[axis = tensor<int32, []>(0), name = tensor<string, []>("electra_encoder_layer_9_attention_self_value_weight_affine_quantized"), quantized_data = tensor<int8, [256, 256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(11550976))), scale = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(11616896))), zero_point = tensor<int8, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(11616576)))];
            tensor<fp32, [256]> electra_encoder_layer_9_attention_output_dense_bias = const()[name = tensor<string, []>("electra_encoder_layer_9_attention_output_dense_bias"), val = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(11617984)))];
            tensor<fp32, [256, 256]> electra_encoder_layer_9_attention_output_dense_weight_affine_quantized = constexpr_affine_dequantize()[axis = tensor<int32, []>(0), name = tensor<string, []>("electra_encoder_layer_9_attention_output_dense_weight_affine_quantized"), quantized_data = tensor<int8, [256, 256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(11619072))), scale = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(11684992))), zero_point = tensor<int8, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(11684672)))];
            tensor<fp32, [256]> electra_encoder_layer_9_attention_output_LayerNorm_bias = const()[name = tensor<string, []>("electra_encoder_layer_9_attention_output_LayerNorm_bias"), val = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(11686080)))];
            tensor<fp32, [256]> electra_encoder_layer_9_attention_output_LayerNorm_weight = const()[name = tensor<string, []>("electra_encoder_layer_9_attention_output_LayerNorm_weight"), val = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(11687168)))];
            tensor<fp32, [1024]> electra_encoder_layer_9_intermediate_dense_bias = const()[name = tensor<string, []>("electra_encoder_layer_9_intermediate_dense_bias"), val = tensor<fp32, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(11688256)))];
            tensor<fp32, [1024, 256]> electra_encoder_layer_9_intermediate_dense_weight_affine_quantized = constexpr_affine_dequantize()[axis = tensor<int32, []>(0), name = tensor<string, []>("electra_encoder_layer_9_intermediate_dense_weight_affine_quantized"), quantized_data = tensor<int8, [1024, 256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(11692416))), scale = tensor<fp32, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(11955712))), zero_point = tensor<int8, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(11954624)))];
            tensor<fp32, [256]> electra_encoder_layer_9_output_dense_bias = const()[name = tensor<string, []>("electra_encoder_layer_9_output_dense_bias"), val = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(11959872)))];
            tensor<fp32, [256, 1024]> electra_encoder_layer_9_output_dense_weight_affine_quantized = constexpr_affine_dequantize()[axis = tensor<int32, []>(0), name = tensor<string, []>("electra_encoder_layer_9_output_dense_weight_affine_quantized"), quantized_data = tensor<int8, [256, 1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(11960960))), scale = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(12223488))), zero_point = tensor<int8, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(12223168)))];
            tensor<fp32, [256]> electra_encoder_layer_9_output_LayerNorm_bias = const()[name = tensor<string, []>("electra_encoder_layer_9_output_LayerNorm_bias"), val = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(12224576)))];
            tensor<fp32, [256]> electra_encoder_layer_9_output_LayerNorm_weight = const()[name = tensor<string, []>("electra_encoder_layer_9_output_LayerNorm_weight"), val = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(12225664)))];
            tensor<fp32, [256]> electra_encoder_layer_10_attention_self_query_bias = const()[name = tensor<string, []>("electra_encoder_layer_10_attention_self_query_bias"), val = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(12226752)))];
            tensor<fp32, [256, 256]> electra_encoder_layer_10_attention_self_query_weight_affine_quantized = constexpr_affine_dequantize()[axis = tensor<int32, []>(0), name = tensor<string, []>("electra_encoder_layer_10_attention_self_query_weight_affine_quantized"), quantized_data = tensor<int8, [256, 256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(12227840))), scale = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(12293760))), zero_point = tensor<int8, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(12293440)))];
            tensor<fp32, [256]> electra_encoder_layer_10_attention_self_key_bias = const()[name = tensor<string, []>("electra_encoder_layer_10_attention_self_key_bias"), val = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(12294848)))];
            tensor<fp32, [256, 256]> electra_encoder_layer_10_attention_self_key_weight_affine_quantized = constexpr_affine_dequantize()[axis = tensor<int32, []>(0), name = tensor<string, []>("electra_encoder_layer_10_attention_self_key_weight_affine_quantized"), quantized_data = tensor<int8, [256, 256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(12295936))), scale = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(12361856))), zero_point = tensor<int8, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(12361536)))];
            tensor<fp32, [256]> electra_encoder_layer_10_attention_self_value_bias = const()[name = tensor<string, []>("electra_encoder_layer_10_attention_self_value_bias"), val = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(12362944)))];
            tensor<fp32, [256, 256]> electra_encoder_layer_10_attention_self_value_weight_affine_quantized = constexpr_affine_dequantize()[axis = tensor<int32, []>(0), name = tensor<string, []>("electra_encoder_layer_10_attention_self_value_weight_affine_quantized"), quantized_data = tensor<int8, [256, 256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(12364032))), scale = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(12429952))), zero_point = tensor<int8, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(12429632)))];
            tensor<fp32, [256]> electra_encoder_layer_10_attention_output_dense_bias = const()[name = tensor<string, []>("electra_encoder_layer_10_attention_output_dense_bias"), val = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(12431040)))];
            tensor<fp32, [256, 256]> electra_encoder_layer_10_attention_output_dense_weight_affine_quantized = constexpr_affine_dequantize()[axis = tensor<int32, []>(0), name = tensor<string, []>("electra_encoder_layer_10_attention_output_dense_weight_affine_quantized"), quantized_data = tensor<int8, [256, 256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(12432128))), scale = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(12498048))), zero_point = tensor<int8, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(12497728)))];
            tensor<fp32, [256]> electra_encoder_layer_10_attention_output_LayerNorm_bias = const()[name = tensor<string, []>("electra_encoder_layer_10_attention_output_LayerNorm_bias"), val = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(12499136)))];
            tensor<fp32, [256]> electra_encoder_layer_10_attention_output_LayerNorm_weight = const()[name = tensor<string, []>("electra_encoder_layer_10_attention_output_LayerNorm_weight"), val = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(12500224)))];
            tensor<fp32, [1024]> electra_encoder_layer_10_intermediate_dense_bias = const()[name = tensor<string, []>("electra_encoder_layer_10_intermediate_dense_bias"), val = tensor<fp32, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(12501312)))];
            tensor<fp32, [1024, 256]> electra_encoder_layer_10_intermediate_dense_weight_affine_quantized = constexpr_affine_dequantize()[axis = tensor<int32, []>(0), name = tensor<string, []>("electra_encoder_layer_10_intermediate_dense_weight_affine_quantized"), quantized_data = tensor<int8, [1024, 256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(12505472))), scale = tensor<fp32, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(12768768))), zero_point = tensor<int8, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(12767680)))];
            tensor<fp32, [256]> electra_encoder_layer_10_output_dense_bias = const()[name = tensor<string, []>("electra_encoder_layer_10_output_dense_bias"), val = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(12772928)))];
            tensor<fp32, [256, 1024]> electra_encoder_layer_10_output_dense_weight_affine_quantized = constexpr_affine_dequantize()[axis = tensor<int32, []>(0), name = tensor<string, []>("electra_encoder_layer_10_output_dense_weight_affine_quantized"), quantized_data = tensor<int8, [256, 1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(12774016))), scale = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(13036544))), zero_point = tensor<int8, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(13036224)))];
            tensor<fp32, [256]> electra_encoder_layer_10_output_LayerNorm_bias = const()[name = tensor<string, []>("electra_encoder_layer_10_output_LayerNorm_bias"), val = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(13037632)))];
            tensor<fp32, [256]> electra_encoder_layer_10_output_LayerNorm_weight = const()[name = tensor<string, []>("electra_encoder_layer_10_output_LayerNorm_weight"), val = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(13038720)))];
            tensor<fp32, [256]> electra_encoder_layer_11_attention_self_query_bias = const()[name = tensor<string, []>("electra_encoder_layer_11_attention_self_query_bias"), val = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(13039808)))];
            tensor<fp32, [256, 256]> electra_encoder_layer_11_attention_self_query_weight_affine_quantized = constexpr_affine_dequantize()[axis = tensor<int32, []>(0), name = tensor<string, []>("electra_encoder_layer_11_attention_self_query_weight_affine_quantized"), quantized_data = tensor<int8, [256, 256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(13040896))), scale = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(13106816))), zero_point = tensor<int8, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(13106496)))];
            tensor<fp32, [256]> electra_encoder_layer_11_attention_self_key_bias = const()[name = tensor<string, []>("electra_encoder_layer_11_attention_self_key_bias"), val = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(13107904)))];
            tensor<fp32, [256, 256]> electra_encoder_layer_11_attention_self_key_weight_affine_quantized = constexpr_affine_dequantize()[axis = tensor<int32, []>(0), name = tensor<string, []>("electra_encoder_layer_11_attention_self_key_weight_affine_quantized"), quantized_data = tensor<int8, [256, 256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(13108992))), scale = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(13174912))), zero_point = tensor<int8, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(13174592)))];
            tensor<fp32, [256]> electra_encoder_layer_11_attention_self_value_bias = const()[name = tensor<string, []>("electra_encoder_layer_11_attention_self_value_bias"), val = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(13176000)))];
            tensor<fp32, [256, 256]> electra_encoder_layer_11_attention_self_value_weight_affine_quantized = constexpr_affine_dequantize()[axis = tensor<int32, []>(0), name = tensor<string, []>("electra_encoder_layer_11_attention_self_value_weight_affine_quantized"), quantized_data = tensor<int8, [256, 256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(13177088))), scale = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(13243008))), zero_point = tensor<int8, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(13242688)))];
            tensor<fp32, [256]> electra_encoder_layer_11_attention_output_dense_bias = const()[name = tensor<string, []>("electra_encoder_layer_11_attention_output_dense_bias"), val = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(13244096)))];
            tensor<fp32, [256, 256]> electra_encoder_layer_11_attention_output_dense_weight_affine_quantized = constexpr_affine_dequantize()[axis = tensor<int32, []>(0), name = tensor<string, []>("electra_encoder_layer_11_attention_output_dense_weight_affine_quantized"), quantized_data = tensor<int8, [256, 256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(13245184))), scale = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(13311104))), zero_point = tensor<int8, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(13310784)))];
            tensor<fp32, [256]> electra_encoder_layer_11_attention_output_LayerNorm_bias = const()[name = tensor<string, []>("electra_encoder_layer_11_attention_output_LayerNorm_bias"), val = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(13312192)))];
            tensor<fp32, [256]> electra_encoder_layer_11_attention_output_LayerNorm_weight = const()[name = tensor<string, []>("electra_encoder_layer_11_attention_output_LayerNorm_weight"), val = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(13313280)))];
            tensor<fp32, [1024]> electra_encoder_layer_11_intermediate_dense_bias = const()[name = tensor<string, []>("electra_encoder_layer_11_intermediate_dense_bias"), val = tensor<fp32, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(13314368)))];
            tensor<fp32, [1024, 256]> electra_encoder_layer_11_intermediate_dense_weight_affine_quantized = constexpr_affine_dequantize()[axis = tensor<int32, []>(0), name = tensor<string, []>("electra_encoder_layer_11_intermediate_dense_weight_affine_quantized"), quantized_data = tensor<int8, [1024, 256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(13318528))), scale = tensor<fp32, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(13581824))), zero_point = tensor<int8, [1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(13580736)))];
            tensor<fp32, [256]> electra_encoder_layer_11_output_dense_bias = const()[name = tensor<string, []>("electra_encoder_layer_11_output_dense_bias"), val = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(13585984)))];
            tensor<fp32, [256, 1024]> electra_encoder_layer_11_output_dense_weight_affine_quantized = constexpr_affine_dequantize()[axis = tensor<int32, []>(0), name = tensor<string, []>("electra_encoder_layer_11_output_dense_weight_affine_quantized"), quantized_data = tensor<int8, [256, 1024]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(13587072))), scale = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(13849600))), zero_point = tensor<int8, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(13849280)))];
            tensor<fp32, [256]> electra_encoder_layer_11_output_LayerNorm_bias = const()[name = tensor<string, []>("electra_encoder_layer_11_output_LayerNorm_bias"), val = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(13850688)))];
            tensor<fp32, [256]> electra_encoder_layer_11_output_LayerNorm_weight = const()[name = tensor<string, []>("electra_encoder_layer_11_output_LayerNorm_weight"), val = tensor<fp32, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(13851776)))];
            tensor<fp32, [128]> sequence_classifier_0_bias = const()[name = tensor<string, []>("sequence_classifier_0_bias"), val = tensor<fp32, [128]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(13852864)))];
            tensor<fp32, [128, 256]> sequence_classifier_0_weight_affine_quantized = constexpr_affine_dequantize()[axis = tensor<int32, []>(0), name = tensor<string, []>("sequence_classifier_0_weight_affine_quantized"), quantized_data = tensor<int8, [128, 256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(13853440))), scale = tensor<fp32, [128]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(13886464))), zero_point = tensor<int8, [128]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(13886272)))];
            tensor<fp32, [2]> sequence_classifier_3_bias = const()[name = tensor<string, []>("sequence_classifier_3_bias"), val = tensor<fp32, [2]>([0x1.cada2ap-5, -0x1.9b1eecp-5])];
            tensor<fp32, [2, 128]> sequence_classifier_3_weight = const()[name = tensor<string, []>("sequence_classifier_3_weight"), val = tensor<fp32, [2, 128]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(13887040)))];
            tensor<fp32, [128]> token_classifier_0_bias = const()[name = tensor<string, []>("token_classifier_0_bias"), val = tensor<fp32, [128]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(13888128)))];
            tensor<fp32, [128, 256]> token_classifier_0_weight_affine_quantized = constexpr_affine_dequantize()[axis = tensor<int32, []>(0), name = tensor<string, []>("token_classifier_0_weight_affine_quantized"), quantized_data = tensor<int8, [128, 256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(13888704))), scale = tensor<fp32, [128]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(13921728))), zero_point = tensor<int8, [128]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(13921536)))];
            tensor<fp32, [3]> token_classifier_3_bias = const()[name = tensor<string, []>("token_classifier_3_bias"), val = tensor<fp32, [3]>([0x1.0ed7f4p-4, -0x1.4b9e4p-5, -0x1.c27252p-5])];
            tensor<fp32, [3, 128]> token_classifier_3_weight = const()[name = tensor<string, []>("token_classifier_3_weight"), val = tensor<fp32, [3, 128]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(13922304)))];
            tensor<string, []> input_ids_dtype_0 = const()[name = tensor<string, []>("input_ids_dtype_0"), val = tensor<string, []>("int32")];
            tensor<string, []> attention_mask_3_dtype_0 = const()[name = tensor<string, []>("attention_mask_3_dtype_0"), val = tensor<string, []>("int32")];
            tensor<int32, []> var_21 = const()[name = tensor<string, []>("op_21"), val = tensor<int32, []>(-1)];
            tensor<fp32, []> var_23 = const()[name = tensor<string, []>("op_23"), val = tensor<fp32, []>(0x1.197998p-40)];
            tensor<fp32, []> var_26 = const()[name = tensor<string, []>("op_26"), val = tensor<fp32, []>(0x1p+0)];
            tensor<int32, [1]> var_47_axes_0 = const()[name = tensor<string, []>("op_47_axes_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, [1, 512]> cast_2 = cast(dtype = attention_mask_3_dtype_0, x = attention_mask)[name = tensor<string, []>("cast_2")];
            tensor<int32, [1, 1, 512]> var_47 = expand_dims(axes = var_47_axes_0, x = cast_2)[name = tensor<string, []>("op_47")];
            tensor<int32, [1]> var_48_axes_0 = const()[name = tensor<string, []>("op_48_axes_0"), val = tensor<int32, [1]>([2])];
            tensor<int32, [1, 1, 1, 512]> var_48 = expand_dims(axes = var_48_axes_0, x = var_47)[name = tensor<string, []>("op_48")];
            tensor<string, []> var_50_dtype_0 = const()[name = tensor<string, []>("op_50_dtype_0"), val = tensor<string, []>("fp32")];
            tensor<fp32, [1, 1, 1, 512]> cast_1 = cast(dtype = var_50_dtype_0, x = var_48)[name = tensor<string, []>("cast_1")];
            tensor<fp32, [1, 1, 1, 512]> var_51 = sub(x = var_26, y = cast_1)[name = tensor<string, []>("op_51")];
            tensor<fp32, []> var_52 = const()[name = tensor<string, []>("op_52"), val = tensor<fp32, []>(-0x1.fffffep+127)];
            tensor<fp32, [1, 1, 1, 512]> attention_mask_1 = mul(x = var_51, y = var_52)[name = tensor<string, []>("attention_mask")];
            tensor<int32, []> inputs_embeds_axis_0 = const()[name = tensor<string, []>("inputs_embeds_axis_0"), val = tensor<int32, []>(0)];
            tensor<int32, []> inputs_embeds_batch_dims_0 = const()[name = tensor<string, []>("inputs_embeds_batch_dims_0"), val = tensor<int32, []>(0)];
            tensor<int32, [1, 512]> cast_0 = cast(dtype = input_ids_dtype_0, x = input_ids)[name = tensor<string, []>("cast_0")];
            tensor<fp32, [1, 512, 128]> inputs_embeds = gather(axis = inputs_embeds_axis_0, batch_dims = inputs_embeds_batch_dims_0, indices = cast_0, x = electra_embeddings_word_embeddings_weight_affine_quantized)[name = tensor<string, []>("inputs_embeds")];
            tensor<fp32, [1, 512, 128]> token_type_embeddings_1_affine_quantized = constexpr_affine_dequantize()[axis = tensor<int32, []>(0), name = tensor<string, []>("token_type_embeddings_1_affine_quantized"), quantized_data = tensor<int8, [1, 512, 128]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(13923904))), scale = tensor<fp32, []>(0x1.25193cp-11), zero_point = tensor<int8, []>(-6)];
            tensor<fp32, [1, 512, 128]> embeddings_1 = add(x = inputs_embeds, y = token_type_embeddings_1_affine_quantized)[name = tensor<string, []>("embeddings_1")];
            tensor<fp32, [1, 512, 128]> position_embeddings_1_affine_quantized = constexpr_affine_dequantize()[axis = tensor<int32, []>(0), name = tensor<string, []>("position_embeddings_1_affine_quantized"), quantized_data = tensor<int8, [1, 512, 128]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(13989504))), scale = tensor<fp32, []>(0x1.2c0c22p-9), zero_point = tensor<int8, []>(29)];
            tensor<fp32, [1, 512, 128]> input_5 = add(x = embeddings_1, y = position_embeddings_1_affine_quantized)[name = tensor<string, []>("input_5")];
            tensor<int32, [1]> input_7_axes_0 = const()[name = tensor<string, []>("input_7_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp32, [1, 512, 128]> input_7 = layer_norm(axes = input_7_axes_0, beta = electra_embeddings_LayerNorm_bias, epsilon = var_23, gamma = electra_embeddings_LayerNorm_weight, x = input_5)[name = tensor<string, []>("input_7")];
            tensor<fp32, [1, 512, 256]> linear_0 = linear(bias = electra_embeddings_project_bias, weight = electra_embeddings_project_weight_affine_quantized, x = input_7)[name = tensor<string, []>("linear_0")];
            tensor<fp32, [1, 512, 256]> linear_1 = linear(bias = electra_encoder_layer_0_attention_self_query_bias, weight = electra_encoder_layer_0_attention_self_query_weight_affine_quantized, x = linear_0)[name = tensor<string, []>("linear_1")];
            tensor<fp32, [1, 512, 256]> linear_2 = linear(bias = electra_encoder_layer_0_attention_self_key_bias, weight = electra_encoder_layer_0_attention_self_key_weight_affine_quantized, x = linear_0)[name = tensor<string, []>("linear_2")];
            tensor<int32, [4]> var_122 = const()[name = tensor<string, []>("op_122"), val = tensor<int32, [4]>([1, 512, 4, 64])];
            tensor<fp32, [1, 512, 4, 64]> x_3 = reshape(shape = var_122, x = linear_2)[name = tensor<string, []>("x_3")];
            tensor<fp32, [1, 512, 256]> linear_3 = linear(bias = electra_encoder_layer_0_attention_self_value_bias, weight = electra_encoder_layer_0_attention_self_value_weight_affine_quantized, x = linear_0)[name = tensor<string, []>("linear_3")];
            tensor<int32, [4]> var_131 = const()[name = tensor<string, []>("op_131"), val = tensor<int32, [4]>([1, 512, 4, 64])];
            tensor<fp32, [1, 512, 4, 64]> x_7 = reshape(shape = var_131, x = linear_3)[name = tensor<string, []>("x_7")];
            tensor<int32, [4]> var_133 = const()[name = tensor<string, []>("op_133"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [4]> var_137 = const()[name = tensor<string, []>("op_137"), val = tensor<int32, [4]>([1, 512, 4, 64])];
            tensor<fp32, [1, 512, 4, 64]> x_11 = reshape(shape = var_137, x = linear_1)[name = tensor<string, []>("x_11")];
            tensor<bool, []> attention_scores_1_transpose_x_0 = const()[name = tensor<string, []>("attention_scores_1_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> attention_scores_1_transpose_y_0 = const()[name = tensor<string, []>("attention_scores_1_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<int32, [4]> transpose_48_perm_0 = const()[name = tensor<string, []>("transpose_48_perm_0"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [4]> transpose_49_perm_0 = const()[name = tensor<string, []>("transpose_49_perm_0"), val = tensor<int32, [4]>([0, 2, 3, 1])];
            tensor<fp32, [1, 4, 64, 512]> transpose_118 = transpose(perm = transpose_49_perm_0, x = x_3)[name = tensor<string, []>("transpose_118")];
            tensor<fp32, [1, 4, 512, 64]> transpose_119 = transpose(perm = transpose_48_perm_0, x = x_11)[name = tensor<string, []>("transpose_119")];
            tensor<fp32, [1, 4, 512, 512]> attention_scores_1 = matmul(transpose_x = attention_scores_1_transpose_x_0, transpose_y = attention_scores_1_transpose_y_0, x = transpose_119, y = transpose_118)[name = tensor<string, []>("attention_scores_1")];
            tensor<fp32, []> _inversed_attention_scores_3_y_0 = const()[name = tensor<string, []>("_inversed_attention_scores_3_y_0"), val = tensor<fp32, []>(0x1p-3)];
            tensor<fp32, [1, 4, 512, 512]> _inversed_attention_scores_3 = mul(x = attention_scores_1, y = _inversed_attention_scores_3_y_0)[name = tensor<string, []>("_inversed_attention_scores_3")];
            tensor<fp32, [1, 4, 512, 512]> input_13 = add(x = _inversed_attention_scores_3, y = attention_mask_1)[name = tensor<string, []>("input_13")];
            tensor<fp32, [1, 4, 512, 512]> input_15 = softmax(axis = var_21, x = input_13)[name = tensor<string, []>("input_15")];
            tensor<bool, []> context_layer_1_transpose_x_0 = const()[name = tensor<string, []>("context_layer_1_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> context_layer_1_transpose_y_0 = const()[name = tensor<string, []>("context_layer_1_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<fp32, [1, 4, 512, 64]> transpose_117 = transpose(perm = var_133, x = x_7)[name = tensor<string, []>("transpose_117")];
            tensor<fp32, [1, 4, 512, 64]> context_layer_1 = matmul(transpose_x = context_layer_1_transpose_x_0, transpose_y = context_layer_1_transpose_y_0, x = input_15, y = transpose_117)[name = tensor<string, []>("context_layer_1")];
            tensor<int32, [4]> var_149 = const()[name = tensor<string, []>("op_149"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [3]> var_154 = const()[name = tensor<string, []>("op_154"), val = tensor<int32, [3]>([1, 512, 256])];
            tensor<fp32, [1, 512, 4, 64]> transpose_116 = transpose(perm = var_149, x = context_layer_1)[name = tensor<string, []>("transpose_116")];
            tensor<fp32, [1, 512, 256]> input_17 = reshape(shape = var_154, x = transpose_116)[name = tensor<string, []>("input_17")];
            tensor<fp32, [1, 512, 256]> linear_4 = linear(bias = electra_encoder_layer_0_attention_output_dense_bias, weight = electra_encoder_layer_0_attention_output_dense_weight_affine_quantized, x = input_17)[name = tensor<string, []>("linear_4")];
            tensor<fp32, [1, 512, 256]> input_21 = add(x = linear_4, y = linear_0)[name = tensor<string, []>("input_21")];
            tensor<int32, [1]> input_23_axes_0 = const()[name = tensor<string, []>("input_23_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp32, [1, 512, 256]> input_23 = layer_norm(axes = input_23_axes_0, beta = electra_encoder_layer_0_attention_output_LayerNorm_bias, epsilon = var_23, gamma = electra_encoder_layer_0_attention_output_LayerNorm_weight, x = input_21)[name = tensor<string, []>("input_23")];
            tensor<fp32, [1, 512, 1024]> linear_5 = linear(bias = electra_encoder_layer_0_intermediate_dense_bias, weight = electra_encoder_layer_0_intermediate_dense_weight_affine_quantized, x = input_23)[name = tensor<string, []>("linear_5")];
            tensor<string, []> input_27_mode_0 = const()[name = tensor<string, []>("input_27_mode_0"), val = tensor<string, []>("EXACT")];
            tensor<fp32, [1, 512, 1024]> input_27 = gelu(mode = input_27_mode_0, x = linear_5)[name = tensor<string, []>("input_27")];
            tensor<fp32, [1, 512, 256]> linear_6 = linear(bias = electra_encoder_layer_0_output_dense_bias, weight = electra_encoder_layer_0_output_dense_weight_affine_quantized, x = input_27)[name = tensor<string, []>("linear_6")];
            tensor<fp32, [1, 512, 256]> input_31 = add(x = linear_6, y = input_23)[name = tensor<string, []>("input_31")];
            tensor<int32, [1]> input_33_axes_0 = const()[name = tensor<string, []>("input_33_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp32, [1, 512, 256]> input_33 = layer_norm(axes = input_33_axes_0, beta = electra_encoder_layer_0_output_LayerNorm_bias, epsilon = var_23, gamma = electra_encoder_layer_0_output_LayerNorm_weight, x = input_31)[name = tensor<string, []>("input_33")];
            tensor<fp32, [1, 512, 256]> linear_7 = linear(bias = electra_encoder_layer_1_attention_self_query_bias, weight = electra_encoder_layer_1_attention_self_query_weight_affine_quantized, x = input_33)[name = tensor<string, []>("linear_7")];
            tensor<fp32, [1, 512, 256]> linear_8 = linear(bias = electra_encoder_layer_1_attention_self_key_bias, weight = electra_encoder_layer_1_attention_self_key_weight_affine_quantized, x = input_33)[name = tensor<string, []>("linear_8")];
            tensor<int32, [4]> var_199 = const()[name = tensor<string, []>("op_199"), val = tensor<int32, [4]>([1, 512, 4, 64])];
            tensor<fp32, [1, 512, 4, 64]> x_15 = reshape(shape = var_199, x = linear_8)[name = tensor<string, []>("x_15")];
            tensor<fp32, [1, 512, 256]> linear_9 = linear(bias = electra_encoder_layer_1_attention_self_value_bias, weight = electra_encoder_layer_1_attention_self_value_weight_affine_quantized, x = input_33)[name = tensor<string, []>("linear_9")];
            tensor<int32, [4]> var_208 = const()[name = tensor<string, []>("op_208"), val = tensor<int32, [4]>([1, 512, 4, 64])];
            tensor<fp32, [1, 512, 4, 64]> x_19 = reshape(shape = var_208, x = linear_9)[name = tensor<string, []>("x_19")];
            tensor<int32, [4]> var_210 = const()[name = tensor<string, []>("op_210"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [4]> var_214 = const()[name = tensor<string, []>("op_214"), val = tensor<int32, [4]>([1, 512, 4, 64])];
            tensor<fp32, [1, 512, 4, 64]> x_23 = reshape(shape = var_214, x = linear_7)[name = tensor<string, []>("x_23")];
            tensor<bool, []> attention_scores_5_transpose_x_0 = const()[name = tensor<string, []>("attention_scores_5_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> attention_scores_5_transpose_y_0 = const()[name = tensor<string, []>("attention_scores_5_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<int32, [4]> transpose_50_perm_0 = const()[name = tensor<string, []>("transpose_50_perm_0"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [4]> transpose_51_perm_0 = const()[name = tensor<string, []>("transpose_51_perm_0"), val = tensor<int32, [4]>([0, 2, 3, 1])];
            tensor<fp32, [1, 4, 64, 512]> transpose_114 = transpose(perm = transpose_51_perm_0, x = x_15)[name = tensor<string, []>("transpose_114")];
            tensor<fp32, [1, 4, 512, 64]> transpose_115 = transpose(perm = transpose_50_perm_0, x = x_23)[name = tensor<string, []>("transpose_115")];
            tensor<fp32, [1, 4, 512, 512]> attention_scores_5 = matmul(transpose_x = attention_scores_5_transpose_x_0, transpose_y = attention_scores_5_transpose_y_0, x = transpose_115, y = transpose_114)[name = tensor<string, []>("attention_scores_5")];
            tensor<fp32, []> _inversed_attention_scores_7_y_0 = const()[name = tensor<string, []>("_inversed_attention_scores_7_y_0"), val = tensor<fp32, []>(0x1p-3)];
            tensor<fp32, [1, 4, 512, 512]> _inversed_attention_scores_7 = mul(x = attention_scores_5, y = _inversed_attention_scores_7_y_0)[name = tensor<string, []>("_inversed_attention_scores_7")];
            tensor<fp32, [1, 4, 512, 512]> input_35 = add(x = _inversed_attention_scores_7, y = attention_mask_1)[name = tensor<string, []>("input_35")];
            tensor<fp32, [1, 4, 512, 512]> input_37 = softmax(axis = var_21, x = input_35)[name = tensor<string, []>("input_37")];
            tensor<bool, []> context_layer_5_transpose_x_0 = const()[name = tensor<string, []>("context_layer_5_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> context_layer_5_transpose_y_0 = const()[name = tensor<string, []>("context_layer_5_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<fp32, [1, 4, 512, 64]> transpose_113 = transpose(perm = var_210, x = x_19)[name = tensor<string, []>("transpose_113")];
            tensor<fp32, [1, 4, 512, 64]> context_layer_5 = matmul(transpose_x = context_layer_5_transpose_x_0, transpose_y = context_layer_5_transpose_y_0, x = input_37, y = transpose_113)[name = tensor<string, []>("context_layer_5")];
            tensor<int32, [4]> var_226 = const()[name = tensor<string, []>("op_226"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [3]> var_231 = const()[name = tensor<string, []>("op_231"), val = tensor<int32, [3]>([1, 512, 256])];
            tensor<fp32, [1, 512, 4, 64]> transpose_112 = transpose(perm = var_226, x = context_layer_5)[name = tensor<string, []>("transpose_112")];
            tensor<fp32, [1, 512, 256]> input_39 = reshape(shape = var_231, x = transpose_112)[name = tensor<string, []>("input_39")];
            tensor<fp32, [1, 512, 256]> linear_10 = linear(bias = electra_encoder_layer_1_attention_output_dense_bias, weight = electra_encoder_layer_1_attention_output_dense_weight_affine_quantized, x = input_39)[name = tensor<string, []>("linear_10")];
            tensor<fp32, [1, 512, 256]> input_43 = add(x = linear_10, y = input_33)[name = tensor<string, []>("input_43")];
            tensor<int32, [1]> input_45_axes_0 = const()[name = tensor<string, []>("input_45_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp32, [1, 512, 256]> input_45 = layer_norm(axes = input_45_axes_0, beta = electra_encoder_layer_1_attention_output_LayerNorm_bias, epsilon = var_23, gamma = electra_encoder_layer_1_attention_output_LayerNorm_weight, x = input_43)[name = tensor<string, []>("input_45")];
            tensor<fp32, [1, 512, 1024]> linear_11 = linear(bias = electra_encoder_layer_1_intermediate_dense_bias, weight = electra_encoder_layer_1_intermediate_dense_weight_affine_quantized, x = input_45)[name = tensor<string, []>("linear_11")];
            tensor<string, []> input_49_mode_0 = const()[name = tensor<string, []>("input_49_mode_0"), val = tensor<string, []>("EXACT")];
            tensor<fp32, [1, 512, 1024]> input_49 = gelu(mode = input_49_mode_0, x = linear_11)[name = tensor<string, []>("input_49")];
            tensor<fp32, [1, 512, 256]> linear_12 = linear(bias = electra_encoder_layer_1_output_dense_bias, weight = electra_encoder_layer_1_output_dense_weight_affine_quantized, x = input_49)[name = tensor<string, []>("linear_12")];
            tensor<fp32, [1, 512, 256]> input_53 = add(x = linear_12, y = input_45)[name = tensor<string, []>("input_53")];
            tensor<int32, [1]> input_55_axes_0 = const()[name = tensor<string, []>("input_55_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp32, [1, 512, 256]> input_55 = layer_norm(axes = input_55_axes_0, beta = electra_encoder_layer_1_output_LayerNorm_bias, epsilon = var_23, gamma = electra_encoder_layer_1_output_LayerNorm_weight, x = input_53)[name = tensor<string, []>("input_55")];
            tensor<fp32, [1, 512, 256]> linear_13 = linear(bias = electra_encoder_layer_2_attention_self_query_bias, weight = electra_encoder_layer_2_attention_self_query_weight_affine_quantized, x = input_55)[name = tensor<string, []>("linear_13")];
            tensor<fp32, [1, 512, 256]> linear_14 = linear(bias = electra_encoder_layer_2_attention_self_key_bias, weight = electra_encoder_layer_2_attention_self_key_weight_affine_quantized, x = input_55)[name = tensor<string, []>("linear_14")];
            tensor<int32, [4]> var_276 = const()[name = tensor<string, []>("op_276"), val = tensor<int32, [4]>([1, 512, 4, 64])];
            tensor<fp32, [1, 512, 4, 64]> x_27 = reshape(shape = var_276, x = linear_14)[name = tensor<string, []>("x_27")];
            tensor<fp32, [1, 512, 256]> linear_15 = linear(bias = electra_encoder_layer_2_attention_self_value_bias, weight = electra_encoder_layer_2_attention_self_value_weight_affine_quantized, x = input_55)[name = tensor<string, []>("linear_15")];
            tensor<int32, [4]> var_285 = const()[name = tensor<string, []>("op_285"), val = tensor<int32, [4]>([1, 512, 4, 64])];
            tensor<fp32, [1, 512, 4, 64]> x_31 = reshape(shape = var_285, x = linear_15)[name = tensor<string, []>("x_31")];
            tensor<int32, [4]> var_287 = const()[name = tensor<string, []>("op_287"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [4]> var_291 = const()[name = tensor<string, []>("op_291"), val = tensor<int32, [4]>([1, 512, 4, 64])];
            tensor<fp32, [1, 512, 4, 64]> x_35 = reshape(shape = var_291, x = linear_13)[name = tensor<string, []>("x_35")];
            tensor<bool, []> attention_scores_9_transpose_x_0 = const()[name = tensor<string, []>("attention_scores_9_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> attention_scores_9_transpose_y_0 = const()[name = tensor<string, []>("attention_scores_9_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<int32, [4]> transpose_52_perm_0 = const()[name = tensor<string, []>("transpose_52_perm_0"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [4]> transpose_53_perm_0 = const()[name = tensor<string, []>("transpose_53_perm_0"), val = tensor<int32, [4]>([0, 2, 3, 1])];
            tensor<fp32, [1, 4, 64, 512]> transpose_110 = transpose(perm = transpose_53_perm_0, x = x_27)[name = tensor<string, []>("transpose_110")];
            tensor<fp32, [1, 4, 512, 64]> transpose_111 = transpose(perm = transpose_52_perm_0, x = x_35)[name = tensor<string, []>("transpose_111")];
            tensor<fp32, [1, 4, 512, 512]> attention_scores_9 = matmul(transpose_x = attention_scores_9_transpose_x_0, transpose_y = attention_scores_9_transpose_y_0, x = transpose_111, y = transpose_110)[name = tensor<string, []>("attention_scores_9")];
            tensor<fp32, []> _inversed_attention_scores_11_y_0 = const()[name = tensor<string, []>("_inversed_attention_scores_11_y_0"), val = tensor<fp32, []>(0x1p-3)];
            tensor<fp32, [1, 4, 512, 512]> _inversed_attention_scores_11 = mul(x = attention_scores_9, y = _inversed_attention_scores_11_y_0)[name = tensor<string, []>("_inversed_attention_scores_11")];
            tensor<fp32, [1, 4, 512, 512]> input_57 = add(x = _inversed_attention_scores_11, y = attention_mask_1)[name = tensor<string, []>("input_57")];
            tensor<fp32, [1, 4, 512, 512]> input_59 = softmax(axis = var_21, x = input_57)[name = tensor<string, []>("input_59")];
            tensor<bool, []> context_layer_9_transpose_x_0 = const()[name = tensor<string, []>("context_layer_9_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> context_layer_9_transpose_y_0 = const()[name = tensor<string, []>("context_layer_9_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<fp32, [1, 4, 512, 64]> transpose_109 = transpose(perm = var_287, x = x_31)[name = tensor<string, []>("transpose_109")];
            tensor<fp32, [1, 4, 512, 64]> context_layer_9 = matmul(transpose_x = context_layer_9_transpose_x_0, transpose_y = context_layer_9_transpose_y_0, x = input_59, y = transpose_109)[name = tensor<string, []>("context_layer_9")];
            tensor<int32, [4]> var_303 = const()[name = tensor<string, []>("op_303"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [3]> var_308 = const()[name = tensor<string, []>("op_308"), val = tensor<int32, [3]>([1, 512, 256])];
            tensor<fp32, [1, 512, 4, 64]> transpose_108 = transpose(perm = var_303, x = context_layer_9)[name = tensor<string, []>("transpose_108")];
            tensor<fp32, [1, 512, 256]> input_61 = reshape(shape = var_308, x = transpose_108)[name = tensor<string, []>("input_61")];
            tensor<fp32, [1, 512, 256]> linear_16 = linear(bias = electra_encoder_layer_2_attention_output_dense_bias, weight = electra_encoder_layer_2_attention_output_dense_weight_affine_quantized, x = input_61)[name = tensor<string, []>("linear_16")];
            tensor<fp32, [1, 512, 256]> input_65 = add(x = linear_16, y = input_55)[name = tensor<string, []>("input_65")];
            tensor<int32, [1]> input_67_axes_0 = const()[name = tensor<string, []>("input_67_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp32, [1, 512, 256]> input_67 = layer_norm(axes = input_67_axes_0, beta = electra_encoder_layer_2_attention_output_LayerNorm_bias, epsilon = var_23, gamma = electra_encoder_layer_2_attention_output_LayerNorm_weight, x = input_65)[name = tensor<string, []>("input_67")];
            tensor<fp32, [1, 512, 1024]> linear_17 = linear(bias = electra_encoder_layer_2_intermediate_dense_bias, weight = electra_encoder_layer_2_intermediate_dense_weight_affine_quantized, x = input_67)[name = tensor<string, []>("linear_17")];
            tensor<string, []> input_71_mode_0 = const()[name = tensor<string, []>("input_71_mode_0"), val = tensor<string, []>("EXACT")];
            tensor<fp32, [1, 512, 1024]> input_71 = gelu(mode = input_71_mode_0, x = linear_17)[name = tensor<string, []>("input_71")];
            tensor<fp32, [1, 512, 256]> linear_18 = linear(bias = electra_encoder_layer_2_output_dense_bias, weight = electra_encoder_layer_2_output_dense_weight_affine_quantized, x = input_71)[name = tensor<string, []>("linear_18")];
            tensor<fp32, [1, 512, 256]> input_75 = add(x = linear_18, y = input_67)[name = tensor<string, []>("input_75")];
            tensor<int32, [1]> input_77_axes_0 = const()[name = tensor<string, []>("input_77_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp32, [1, 512, 256]> input_77 = layer_norm(axes = input_77_axes_0, beta = electra_encoder_layer_2_output_LayerNorm_bias, epsilon = var_23, gamma = electra_encoder_layer_2_output_LayerNorm_weight, x = input_75)[name = tensor<string, []>("input_77")];
            tensor<fp32, [1, 512, 256]> linear_19 = linear(bias = electra_encoder_layer_3_attention_self_query_bias, weight = electra_encoder_layer_3_attention_self_query_weight_affine_quantized, x = input_77)[name = tensor<string, []>("linear_19")];
            tensor<fp32, [1, 512, 256]> linear_20 = linear(bias = electra_encoder_layer_3_attention_self_key_bias, weight = electra_encoder_layer_3_attention_self_key_weight_affine_quantized, x = input_77)[name = tensor<string, []>("linear_20")];
            tensor<int32, [4]> var_353 = const()[name = tensor<string, []>("op_353"), val = tensor<int32, [4]>([1, 512, 4, 64])];
            tensor<fp32, [1, 512, 4, 64]> x_39 = reshape(shape = var_353, x = linear_20)[name = tensor<string, []>("x_39")];
            tensor<fp32, [1, 512, 256]> linear_21 = linear(bias = electra_encoder_layer_3_attention_self_value_bias, weight = electra_encoder_layer_3_attention_self_value_weight_affine_quantized, x = input_77)[name = tensor<string, []>("linear_21")];
            tensor<int32, [4]> var_362 = const()[name = tensor<string, []>("op_362"), val = tensor<int32, [4]>([1, 512, 4, 64])];
            tensor<fp32, [1, 512, 4, 64]> x_43 = reshape(shape = var_362, x = linear_21)[name = tensor<string, []>("x_43")];
            tensor<int32, [4]> var_364 = const()[name = tensor<string, []>("op_364"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [4]> var_368 = const()[name = tensor<string, []>("op_368"), val = tensor<int32, [4]>([1, 512, 4, 64])];
            tensor<fp32, [1, 512, 4, 64]> x_47 = reshape(shape = var_368, x = linear_19)[name = tensor<string, []>("x_47")];
            tensor<bool, []> attention_scores_13_transpose_x_0 = const()[name = tensor<string, []>("attention_scores_13_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> attention_scores_13_transpose_y_0 = const()[name = tensor<string, []>("attention_scores_13_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<int32, [4]> transpose_54_perm_0 = const()[name = tensor<string, []>("transpose_54_perm_0"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [4]> transpose_55_perm_0 = const()[name = tensor<string, []>("transpose_55_perm_0"), val = tensor<int32, [4]>([0, 2, 3, 1])];
            tensor<fp32, [1, 4, 64, 512]> transpose_106 = transpose(perm = transpose_55_perm_0, x = x_39)[name = tensor<string, []>("transpose_106")];
            tensor<fp32, [1, 4, 512, 64]> transpose_107 = transpose(perm = transpose_54_perm_0, x = x_47)[name = tensor<string, []>("transpose_107")];
            tensor<fp32, [1, 4, 512, 512]> attention_scores_13 = matmul(transpose_x = attention_scores_13_transpose_x_0, transpose_y = attention_scores_13_transpose_y_0, x = transpose_107, y = transpose_106)[name = tensor<string, []>("attention_scores_13")];
            tensor<fp32, []> _inversed_attention_scores_15_y_0 = const()[name = tensor<string, []>("_inversed_attention_scores_15_y_0"), val = tensor<fp32, []>(0x1p-3)];
            tensor<fp32, [1, 4, 512, 512]> _inversed_attention_scores_15 = mul(x = attention_scores_13, y = _inversed_attention_scores_15_y_0)[name = tensor<string, []>("_inversed_attention_scores_15")];
            tensor<fp32, [1, 4, 512, 512]> input_79 = add(x = _inversed_attention_scores_15, y = attention_mask_1)[name = tensor<string, []>("input_79")];
            tensor<fp32, [1, 4, 512, 512]> input_81 = softmax(axis = var_21, x = input_79)[name = tensor<string, []>("input_81")];
            tensor<bool, []> context_layer_13_transpose_x_0 = const()[name = tensor<string, []>("context_layer_13_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> context_layer_13_transpose_y_0 = const()[name = tensor<string, []>("context_layer_13_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<fp32, [1, 4, 512, 64]> transpose_105 = transpose(perm = var_364, x = x_43)[name = tensor<string, []>("transpose_105")];
            tensor<fp32, [1, 4, 512, 64]> context_layer_13 = matmul(transpose_x = context_layer_13_transpose_x_0, transpose_y = context_layer_13_transpose_y_0, x = input_81, y = transpose_105)[name = tensor<string, []>("context_layer_13")];
            tensor<int32, [4]> var_380 = const()[name = tensor<string, []>("op_380"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [3]> var_385 = const()[name = tensor<string, []>("op_385"), val = tensor<int32, [3]>([1, 512, 256])];
            tensor<fp32, [1, 512, 4, 64]> transpose_104 = transpose(perm = var_380, x = context_layer_13)[name = tensor<string, []>("transpose_104")];
            tensor<fp32, [1, 512, 256]> input_83 = reshape(shape = var_385, x = transpose_104)[name = tensor<string, []>("input_83")];
            tensor<fp32, [1, 512, 256]> linear_22 = linear(bias = electra_encoder_layer_3_attention_output_dense_bias, weight = electra_encoder_layer_3_attention_output_dense_weight_affine_quantized, x = input_83)[name = tensor<string, []>("linear_22")];
            tensor<fp32, [1, 512, 256]> input_87 = add(x = linear_22, y = input_77)[name = tensor<string, []>("input_87")];
            tensor<int32, [1]> input_89_axes_0 = const()[name = tensor<string, []>("input_89_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp32, [1, 512, 256]> input_89 = layer_norm(axes = input_89_axes_0, beta = electra_encoder_layer_3_attention_output_LayerNorm_bias, epsilon = var_23, gamma = electra_encoder_layer_3_attention_output_LayerNorm_weight, x = input_87)[name = tensor<string, []>("input_89")];
            tensor<fp32, [1, 512, 1024]> linear_23 = linear(bias = electra_encoder_layer_3_intermediate_dense_bias, weight = electra_encoder_layer_3_intermediate_dense_weight_affine_quantized, x = input_89)[name = tensor<string, []>("linear_23")];
            tensor<string, []> input_93_mode_0 = const()[name = tensor<string, []>("input_93_mode_0"), val = tensor<string, []>("EXACT")];
            tensor<fp32, [1, 512, 1024]> input_93 = gelu(mode = input_93_mode_0, x = linear_23)[name = tensor<string, []>("input_93")];
            tensor<fp32, [1, 512, 256]> linear_24 = linear(bias = electra_encoder_layer_3_output_dense_bias, weight = electra_encoder_layer_3_output_dense_weight_affine_quantized, x = input_93)[name = tensor<string, []>("linear_24")];
            tensor<fp32, [1, 512, 256]> input_97 = add(x = linear_24, y = input_89)[name = tensor<string, []>("input_97")];
            tensor<int32, [1]> input_99_axes_0 = const()[name = tensor<string, []>("input_99_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp32, [1, 512, 256]> input_99 = layer_norm(axes = input_99_axes_0, beta = electra_encoder_layer_3_output_LayerNorm_bias, epsilon = var_23, gamma = electra_encoder_layer_3_output_LayerNorm_weight, x = input_97)[name = tensor<string, []>("input_99")];
            tensor<fp32, [1, 512, 256]> linear_25 = linear(bias = electra_encoder_layer_4_attention_self_query_bias, weight = electra_encoder_layer_4_attention_self_query_weight_affine_quantized, x = input_99)[name = tensor<string, []>("linear_25")];
            tensor<fp32, [1, 512, 256]> linear_26 = linear(bias = electra_encoder_layer_4_attention_self_key_bias, weight = electra_encoder_layer_4_attention_self_key_weight_affine_quantized, x = input_99)[name = tensor<string, []>("linear_26")];
            tensor<int32, [4]> var_430 = const()[name = tensor<string, []>("op_430"), val = tensor<int32, [4]>([1, 512, 4, 64])];
            tensor<fp32, [1, 512, 4, 64]> x_51 = reshape(shape = var_430, x = linear_26)[name = tensor<string, []>("x_51")];
            tensor<fp32, [1, 512, 256]> linear_27 = linear(bias = electra_encoder_layer_4_attention_self_value_bias, weight = electra_encoder_layer_4_attention_self_value_weight_affine_quantized, x = input_99)[name = tensor<string, []>("linear_27")];
            tensor<int32, [4]> var_439 = const()[name = tensor<string, []>("op_439"), val = tensor<int32, [4]>([1, 512, 4, 64])];
            tensor<fp32, [1, 512, 4, 64]> x_55 = reshape(shape = var_439, x = linear_27)[name = tensor<string, []>("x_55")];
            tensor<int32, [4]> var_441 = const()[name = tensor<string, []>("op_441"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [4]> var_445 = const()[name = tensor<string, []>("op_445"), val = tensor<int32, [4]>([1, 512, 4, 64])];
            tensor<fp32, [1, 512, 4, 64]> x_59 = reshape(shape = var_445, x = linear_25)[name = tensor<string, []>("x_59")];
            tensor<bool, []> attention_scores_17_transpose_x_0 = const()[name = tensor<string, []>("attention_scores_17_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> attention_scores_17_transpose_y_0 = const()[name = tensor<string, []>("attention_scores_17_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<int32, [4]> transpose_56_perm_0 = const()[name = tensor<string, []>("transpose_56_perm_0"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [4]> transpose_57_perm_0 = const()[name = tensor<string, []>("transpose_57_perm_0"), val = tensor<int32, [4]>([0, 2, 3, 1])];
            tensor<fp32, [1, 4, 64, 512]> transpose_102 = transpose(perm = transpose_57_perm_0, x = x_51)[name = tensor<string, []>("transpose_102")];
            tensor<fp32, [1, 4, 512, 64]> transpose_103 = transpose(perm = transpose_56_perm_0, x = x_59)[name = tensor<string, []>("transpose_103")];
            tensor<fp32, [1, 4, 512, 512]> attention_scores_17 = matmul(transpose_x = attention_scores_17_transpose_x_0, transpose_y = attention_scores_17_transpose_y_0, x = transpose_103, y = transpose_102)[name = tensor<string, []>("attention_scores_17")];
            tensor<fp32, []> _inversed_attention_scores_19_y_0 = const()[name = tensor<string, []>("_inversed_attention_scores_19_y_0"), val = tensor<fp32, []>(0x1p-3)];
            tensor<fp32, [1, 4, 512, 512]> _inversed_attention_scores_19 = mul(x = attention_scores_17, y = _inversed_attention_scores_19_y_0)[name = tensor<string, []>("_inversed_attention_scores_19")];
            tensor<fp32, [1, 4, 512, 512]> input_101 = add(x = _inversed_attention_scores_19, y = attention_mask_1)[name = tensor<string, []>("input_101")];
            tensor<fp32, [1, 4, 512, 512]> input_103 = softmax(axis = var_21, x = input_101)[name = tensor<string, []>("input_103")];
            tensor<bool, []> context_layer_17_transpose_x_0 = const()[name = tensor<string, []>("context_layer_17_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> context_layer_17_transpose_y_0 = const()[name = tensor<string, []>("context_layer_17_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<fp32, [1, 4, 512, 64]> transpose_101 = transpose(perm = var_441, x = x_55)[name = tensor<string, []>("transpose_101")];
            tensor<fp32, [1, 4, 512, 64]> context_layer_17 = matmul(transpose_x = context_layer_17_transpose_x_0, transpose_y = context_layer_17_transpose_y_0, x = input_103, y = transpose_101)[name = tensor<string, []>("context_layer_17")];
            tensor<int32, [4]> var_457 = const()[name = tensor<string, []>("op_457"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [3]> var_462 = const()[name = tensor<string, []>("op_462"), val = tensor<int32, [3]>([1, 512, 256])];
            tensor<fp32, [1, 512, 4, 64]> transpose_100 = transpose(perm = var_457, x = context_layer_17)[name = tensor<string, []>("transpose_100")];
            tensor<fp32, [1, 512, 256]> input_105 = reshape(shape = var_462, x = transpose_100)[name = tensor<string, []>("input_105")];
            tensor<fp32, [1, 512, 256]> linear_28 = linear(bias = electra_encoder_layer_4_attention_output_dense_bias, weight = electra_encoder_layer_4_attention_output_dense_weight_affine_quantized, x = input_105)[name = tensor<string, []>("linear_28")];
            tensor<fp32, [1, 512, 256]> input_109 = add(x = linear_28, y = input_99)[name = tensor<string, []>("input_109")];
            tensor<int32, [1]> input_111_axes_0 = const()[name = tensor<string, []>("input_111_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp32, [1, 512, 256]> input_111 = layer_norm(axes = input_111_axes_0, beta = electra_encoder_layer_4_attention_output_LayerNorm_bias, epsilon = var_23, gamma = electra_encoder_layer_4_attention_output_LayerNorm_weight, x = input_109)[name = tensor<string, []>("input_111")];
            tensor<fp32, [1, 512, 1024]> linear_29 = linear(bias = electra_encoder_layer_4_intermediate_dense_bias, weight = electra_encoder_layer_4_intermediate_dense_weight_affine_quantized, x = input_111)[name = tensor<string, []>("linear_29")];
            tensor<string, []> input_115_mode_0 = const()[name = tensor<string, []>("input_115_mode_0"), val = tensor<string, []>("EXACT")];
            tensor<fp32, [1, 512, 1024]> input_115 = gelu(mode = input_115_mode_0, x = linear_29)[name = tensor<string, []>("input_115")];
            tensor<fp32, [1, 512, 256]> linear_30 = linear(bias = electra_encoder_layer_4_output_dense_bias, weight = electra_encoder_layer_4_output_dense_weight_affine_quantized, x = input_115)[name = tensor<string, []>("linear_30")];
            tensor<fp32, [1, 512, 256]> input_119 = add(x = linear_30, y = input_111)[name = tensor<string, []>("input_119")];
            tensor<int32, [1]> input_121_axes_0 = const()[name = tensor<string, []>("input_121_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp32, [1, 512, 256]> input_121 = layer_norm(axes = input_121_axes_0, beta = electra_encoder_layer_4_output_LayerNorm_bias, epsilon = var_23, gamma = electra_encoder_layer_4_output_LayerNorm_weight, x = input_119)[name = tensor<string, []>("input_121")];
            tensor<fp32, [1, 512, 256]> linear_31 = linear(bias = electra_encoder_layer_5_attention_self_query_bias, weight = electra_encoder_layer_5_attention_self_query_weight_affine_quantized, x = input_121)[name = tensor<string, []>("linear_31")];
            tensor<fp32, [1, 512, 256]> linear_32 = linear(bias = electra_encoder_layer_5_attention_self_key_bias, weight = electra_encoder_layer_5_attention_self_key_weight_affine_quantized, x = input_121)[name = tensor<string, []>("linear_32")];
            tensor<int32, [4]> var_507 = const()[name = tensor<string, []>("op_507"), val = tensor<int32, [4]>([1, 512, 4, 64])];
            tensor<fp32, [1, 512, 4, 64]> x_63 = reshape(shape = var_507, x = linear_32)[name = tensor<string, []>("x_63")];
            tensor<fp32, [1, 512, 256]> linear_33 = linear(bias = electra_encoder_layer_5_attention_self_value_bias, weight = electra_encoder_layer_5_attention_self_value_weight_affine_quantized, x = input_121)[name = tensor<string, []>("linear_33")];
            tensor<int32, [4]> var_516 = const()[name = tensor<string, []>("op_516"), val = tensor<int32, [4]>([1, 512, 4, 64])];
            tensor<fp32, [1, 512, 4, 64]> x_67 = reshape(shape = var_516, x = linear_33)[name = tensor<string, []>("x_67")];
            tensor<int32, [4]> var_518 = const()[name = tensor<string, []>("op_518"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [4]> var_522 = const()[name = tensor<string, []>("op_522"), val = tensor<int32, [4]>([1, 512, 4, 64])];
            tensor<fp32, [1, 512, 4, 64]> x_71 = reshape(shape = var_522, x = linear_31)[name = tensor<string, []>("x_71")];
            tensor<bool, []> attention_scores_21_transpose_x_0 = const()[name = tensor<string, []>("attention_scores_21_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> attention_scores_21_transpose_y_0 = const()[name = tensor<string, []>("attention_scores_21_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<int32, [4]> transpose_58_perm_0 = const()[name = tensor<string, []>("transpose_58_perm_0"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [4]> transpose_59_perm_0 = const()[name = tensor<string, []>("transpose_59_perm_0"), val = tensor<int32, [4]>([0, 2, 3, 1])];
            tensor<fp32, [1, 4, 64, 512]> transpose_98 = transpose(perm = transpose_59_perm_0, x = x_63)[name = tensor<string, []>("transpose_98")];
            tensor<fp32, [1, 4, 512, 64]> transpose_99 = transpose(perm = transpose_58_perm_0, x = x_71)[name = tensor<string, []>("transpose_99")];
            tensor<fp32, [1, 4, 512, 512]> attention_scores_21 = matmul(transpose_x = attention_scores_21_transpose_x_0, transpose_y = attention_scores_21_transpose_y_0, x = transpose_99, y = transpose_98)[name = tensor<string, []>("attention_scores_21")];
            tensor<fp32, []> _inversed_attention_scores_23_y_0 = const()[name = tensor<string, []>("_inversed_attention_scores_23_y_0"), val = tensor<fp32, []>(0x1p-3)];
            tensor<fp32, [1, 4, 512, 512]> _inversed_attention_scores_23 = mul(x = attention_scores_21, y = _inversed_attention_scores_23_y_0)[name = tensor<string, []>("_inversed_attention_scores_23")];
            tensor<fp32, [1, 4, 512, 512]> input_123 = add(x = _inversed_attention_scores_23, y = attention_mask_1)[name = tensor<string, []>("input_123")];
            tensor<fp32, [1, 4, 512, 512]> input_125 = softmax(axis = var_21, x = input_123)[name = tensor<string, []>("input_125")];
            tensor<bool, []> context_layer_21_transpose_x_0 = const()[name = tensor<string, []>("context_layer_21_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> context_layer_21_transpose_y_0 = const()[name = tensor<string, []>("context_layer_21_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<fp32, [1, 4, 512, 64]> transpose_97 = transpose(perm = var_518, x = x_67)[name = tensor<string, []>("transpose_97")];
            tensor<fp32, [1, 4, 512, 64]> context_layer_21 = matmul(transpose_x = context_layer_21_transpose_x_0, transpose_y = context_layer_21_transpose_y_0, x = input_125, y = transpose_97)[name = tensor<string, []>("context_layer_21")];
            tensor<int32, [4]> var_534 = const()[name = tensor<string, []>("op_534"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [3]> var_539 = const()[name = tensor<string, []>("op_539"), val = tensor<int32, [3]>([1, 512, 256])];
            tensor<fp32, [1, 512, 4, 64]> transpose_96 = transpose(perm = var_534, x = context_layer_21)[name = tensor<string, []>("transpose_96")];
            tensor<fp32, [1, 512, 256]> input_127 = reshape(shape = var_539, x = transpose_96)[name = tensor<string, []>("input_127")];
            tensor<fp32, [1, 512, 256]> linear_34 = linear(bias = electra_encoder_layer_5_attention_output_dense_bias, weight = electra_encoder_layer_5_attention_output_dense_weight_affine_quantized, x = input_127)[name = tensor<string, []>("linear_34")];
            tensor<fp32, [1, 512, 256]> input_131 = add(x = linear_34, y = input_121)[name = tensor<string, []>("input_131")];
            tensor<int32, [1]> input_133_axes_0 = const()[name = tensor<string, []>("input_133_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp32, [1, 512, 256]> input_133 = layer_norm(axes = input_133_axes_0, beta = electra_encoder_layer_5_attention_output_LayerNorm_bias, epsilon = var_23, gamma = electra_encoder_layer_5_attention_output_LayerNorm_weight, x = input_131)[name = tensor<string, []>("input_133")];
            tensor<fp32, [1, 512, 1024]> linear_35 = linear(bias = electra_encoder_layer_5_intermediate_dense_bias, weight = electra_encoder_layer_5_intermediate_dense_weight_affine_quantized, x = input_133)[name = tensor<string, []>("linear_35")];
            tensor<string, []> input_137_mode_0 = const()[name = tensor<string, []>("input_137_mode_0"), val = tensor<string, []>("EXACT")];
            tensor<fp32, [1, 512, 1024]> input_137 = gelu(mode = input_137_mode_0, x = linear_35)[name = tensor<string, []>("input_137")];
            tensor<fp32, [1, 512, 256]> linear_36 = linear(bias = electra_encoder_layer_5_output_dense_bias, weight = electra_encoder_layer_5_output_dense_weight_affine_quantized, x = input_137)[name = tensor<string, []>("linear_36")];
            tensor<fp32, [1, 512, 256]> input_141 = add(x = linear_36, y = input_133)[name = tensor<string, []>("input_141")];
            tensor<int32, [1]> input_143_axes_0 = const()[name = tensor<string, []>("input_143_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp32, [1, 512, 256]> input_143 = layer_norm(axes = input_143_axes_0, beta = electra_encoder_layer_5_output_LayerNorm_bias, epsilon = var_23, gamma = electra_encoder_layer_5_output_LayerNorm_weight, x = input_141)[name = tensor<string, []>("input_143")];
            tensor<fp32, [1, 512, 256]> linear_37 = linear(bias = electra_encoder_layer_6_attention_self_query_bias, weight = electra_encoder_layer_6_attention_self_query_weight_affine_quantized, x = input_143)[name = tensor<string, []>("linear_37")];
            tensor<fp32, [1, 512, 256]> linear_38 = linear(bias = electra_encoder_layer_6_attention_self_key_bias, weight = electra_encoder_layer_6_attention_self_key_weight_affine_quantized, x = input_143)[name = tensor<string, []>("linear_38")];
            tensor<int32, [4]> var_584 = const()[name = tensor<string, []>("op_584"), val = tensor<int32, [4]>([1, 512, 4, 64])];
            tensor<fp32, [1, 512, 4, 64]> x_75 = reshape(shape = var_584, x = linear_38)[name = tensor<string, []>("x_75")];
            tensor<fp32, [1, 512, 256]> linear_39 = linear(bias = electra_encoder_layer_6_attention_self_value_bias, weight = electra_encoder_layer_6_attention_self_value_weight_affine_quantized, x = input_143)[name = tensor<string, []>("linear_39")];
            tensor<int32, [4]> var_593 = const()[name = tensor<string, []>("op_593"), val = tensor<int32, [4]>([1, 512, 4, 64])];
            tensor<fp32, [1, 512, 4, 64]> x_79 = reshape(shape = var_593, x = linear_39)[name = tensor<string, []>("x_79")];
            tensor<int32, [4]> var_595 = const()[name = tensor<string, []>("op_595"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [4]> var_599 = const()[name = tensor<string, []>("op_599"), val = tensor<int32, [4]>([1, 512, 4, 64])];
            tensor<fp32, [1, 512, 4, 64]> x_83 = reshape(shape = var_599, x = linear_37)[name = tensor<string, []>("x_83")];
            tensor<bool, []> attention_scores_25_transpose_x_0 = const()[name = tensor<string, []>("attention_scores_25_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> attention_scores_25_transpose_y_0 = const()[name = tensor<string, []>("attention_scores_25_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<int32, [4]> transpose_60_perm_0 = const()[name = tensor<string, []>("transpose_60_perm_0"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [4]> transpose_61_perm_0 = const()[name = tensor<string, []>("transpose_61_perm_0"), val = tensor<int32, [4]>([0, 2, 3, 1])];
            tensor<fp32, [1, 4, 64, 512]> transpose_94 = transpose(perm = transpose_61_perm_0, x = x_75)[name = tensor<string, []>("transpose_94")];
            tensor<fp32, [1, 4, 512, 64]> transpose_95 = transpose(perm = transpose_60_perm_0, x = x_83)[name = tensor<string, []>("transpose_95")];
            tensor<fp32, [1, 4, 512, 512]> attention_scores_25 = matmul(transpose_x = attention_scores_25_transpose_x_0, transpose_y = attention_scores_25_transpose_y_0, x = transpose_95, y = transpose_94)[name = tensor<string, []>("attention_scores_25")];
            tensor<fp32, []> _inversed_attention_scores_27_y_0 = const()[name = tensor<string, []>("_inversed_attention_scores_27_y_0"), val = tensor<fp32, []>(0x1p-3)];
            tensor<fp32, [1, 4, 512, 512]> _inversed_attention_scores_27 = mul(x = attention_scores_25, y = _inversed_attention_scores_27_y_0)[name = tensor<string, []>("_inversed_attention_scores_27")];
            tensor<fp32, [1, 4, 512, 512]> input_145 = add(x = _inversed_attention_scores_27, y = attention_mask_1)[name = tensor<string, []>("input_145")];
            tensor<fp32, [1, 4, 512, 512]> input_147 = softmax(axis = var_21, x = input_145)[name = tensor<string, []>("input_147")];
            tensor<bool, []> context_layer_25_transpose_x_0 = const()[name = tensor<string, []>("context_layer_25_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> context_layer_25_transpose_y_0 = const()[name = tensor<string, []>("context_layer_25_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<fp32, [1, 4, 512, 64]> transpose_93 = transpose(perm = var_595, x = x_79)[name = tensor<string, []>("transpose_93")];
            tensor<fp32, [1, 4, 512, 64]> context_layer_25 = matmul(transpose_x = context_layer_25_transpose_x_0, transpose_y = context_layer_25_transpose_y_0, x = input_147, y = transpose_93)[name = tensor<string, []>("context_layer_25")];
            tensor<int32, [4]> var_611 = const()[name = tensor<string, []>("op_611"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [3]> var_616 = const()[name = tensor<string, []>("op_616"), val = tensor<int32, [3]>([1, 512, 256])];
            tensor<fp32, [1, 512, 4, 64]> transpose_92 = transpose(perm = var_611, x = context_layer_25)[name = tensor<string, []>("transpose_92")];
            tensor<fp32, [1, 512, 256]> input_149 = reshape(shape = var_616, x = transpose_92)[name = tensor<string, []>("input_149")];
            tensor<fp32, [1, 512, 256]> linear_40 = linear(bias = electra_encoder_layer_6_attention_output_dense_bias, weight = electra_encoder_layer_6_attention_output_dense_weight_affine_quantized, x = input_149)[name = tensor<string, []>("linear_40")];
            tensor<fp32, [1, 512, 256]> input_153 = add(x = linear_40, y = input_143)[name = tensor<string, []>("input_153")];
            tensor<int32, [1]> input_155_axes_0 = const()[name = tensor<string, []>("input_155_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp32, [1, 512, 256]> input_155 = layer_norm(axes = input_155_axes_0, beta = electra_encoder_layer_6_attention_output_LayerNorm_bias, epsilon = var_23, gamma = electra_encoder_layer_6_attention_output_LayerNorm_weight, x = input_153)[name = tensor<string, []>("input_155")];
            tensor<fp32, [1, 512, 1024]> linear_41 = linear(bias = electra_encoder_layer_6_intermediate_dense_bias, weight = electra_encoder_layer_6_intermediate_dense_weight_affine_quantized, x = input_155)[name = tensor<string, []>("linear_41")];
            tensor<string, []> input_159_mode_0 = const()[name = tensor<string, []>("input_159_mode_0"), val = tensor<string, []>("EXACT")];
            tensor<fp32, [1, 512, 1024]> input_159 = gelu(mode = input_159_mode_0, x = linear_41)[name = tensor<string, []>("input_159")];
            tensor<fp32, [1, 512, 256]> linear_42 = linear(bias = electra_encoder_layer_6_output_dense_bias, weight = electra_encoder_layer_6_output_dense_weight_affine_quantized, x = input_159)[name = tensor<string, []>("linear_42")];
            tensor<fp32, [1, 512, 256]> input_163 = add(x = linear_42, y = input_155)[name = tensor<string, []>("input_163")];
            tensor<int32, [1]> input_165_axes_0 = const()[name = tensor<string, []>("input_165_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp32, [1, 512, 256]> input_165 = layer_norm(axes = input_165_axes_0, beta = electra_encoder_layer_6_output_LayerNorm_bias, epsilon = var_23, gamma = electra_encoder_layer_6_output_LayerNorm_weight, x = input_163)[name = tensor<string, []>("input_165")];
            tensor<fp32, [1, 512, 256]> linear_43 = linear(bias = electra_encoder_layer_7_attention_self_query_bias, weight = electra_encoder_layer_7_attention_self_query_weight_affine_quantized, x = input_165)[name = tensor<string, []>("linear_43")];
            tensor<fp32, [1, 512, 256]> linear_44 = linear(bias = electra_encoder_layer_7_attention_self_key_bias, weight = electra_encoder_layer_7_attention_self_key_weight_affine_quantized, x = input_165)[name = tensor<string, []>("linear_44")];
            tensor<int32, [4]> var_661 = const()[name = tensor<string, []>("op_661"), val = tensor<int32, [4]>([1, 512, 4, 64])];
            tensor<fp32, [1, 512, 4, 64]> x_87 = reshape(shape = var_661, x = linear_44)[name = tensor<string, []>("x_87")];
            tensor<fp32, [1, 512, 256]> linear_45 = linear(bias = electra_encoder_layer_7_attention_self_value_bias, weight = electra_encoder_layer_7_attention_self_value_weight_affine_quantized, x = input_165)[name = tensor<string, []>("linear_45")];
            tensor<int32, [4]> var_670 = const()[name = tensor<string, []>("op_670"), val = tensor<int32, [4]>([1, 512, 4, 64])];
            tensor<fp32, [1, 512, 4, 64]> x_91 = reshape(shape = var_670, x = linear_45)[name = tensor<string, []>("x_91")];
            tensor<int32, [4]> var_672 = const()[name = tensor<string, []>("op_672"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [4]> var_676 = const()[name = tensor<string, []>("op_676"), val = tensor<int32, [4]>([1, 512, 4, 64])];
            tensor<fp32, [1, 512, 4, 64]> x_95 = reshape(shape = var_676, x = linear_43)[name = tensor<string, []>("x_95")];
            tensor<bool, []> attention_scores_29_transpose_x_0 = const()[name = tensor<string, []>("attention_scores_29_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> attention_scores_29_transpose_y_0 = const()[name = tensor<string, []>("attention_scores_29_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<int32, [4]> transpose_62_perm_0 = const()[name = tensor<string, []>("transpose_62_perm_0"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [4]> transpose_63_perm_0 = const()[name = tensor<string, []>("transpose_63_perm_0"), val = tensor<int32, [4]>([0, 2, 3, 1])];
            tensor<fp32, [1, 4, 64, 512]> transpose_90 = transpose(perm = transpose_63_perm_0, x = x_87)[name = tensor<string, []>("transpose_90")];
            tensor<fp32, [1, 4, 512, 64]> transpose_91 = transpose(perm = transpose_62_perm_0, x = x_95)[name = tensor<string, []>("transpose_91")];
            tensor<fp32, [1, 4, 512, 512]> attention_scores_29 = matmul(transpose_x = attention_scores_29_transpose_x_0, transpose_y = attention_scores_29_transpose_y_0, x = transpose_91, y = transpose_90)[name = tensor<string, []>("attention_scores_29")];
            tensor<fp32, []> _inversed_attention_scores_31_y_0 = const()[name = tensor<string, []>("_inversed_attention_scores_31_y_0"), val = tensor<fp32, []>(0x1p-3)];
            tensor<fp32, [1, 4, 512, 512]> _inversed_attention_scores_31 = mul(x = attention_scores_29, y = _inversed_attention_scores_31_y_0)[name = tensor<string, []>("_inversed_attention_scores_31")];
            tensor<fp32, [1, 4, 512, 512]> input_167 = add(x = _inversed_attention_scores_31, y = attention_mask_1)[name = tensor<string, []>("input_167")];
            tensor<fp32, [1, 4, 512, 512]> input_169 = softmax(axis = var_21, x = input_167)[name = tensor<string, []>("input_169")];
            tensor<bool, []> context_layer_29_transpose_x_0 = const()[name = tensor<string, []>("context_layer_29_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> context_layer_29_transpose_y_0 = const()[name = tensor<string, []>("context_layer_29_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<fp32, [1, 4, 512, 64]> transpose_89 = transpose(perm = var_672, x = x_91)[name = tensor<string, []>("transpose_89")];
            tensor<fp32, [1, 4, 512, 64]> context_layer_29 = matmul(transpose_x = context_layer_29_transpose_x_0, transpose_y = context_layer_29_transpose_y_0, x = input_169, y = transpose_89)[name = tensor<string, []>("context_layer_29")];
            tensor<int32, [4]> var_688 = const()[name = tensor<string, []>("op_688"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [3]> var_693 = const()[name = tensor<string, []>("op_693"), val = tensor<int32, [3]>([1, 512, 256])];
            tensor<fp32, [1, 512, 4, 64]> transpose_88 = transpose(perm = var_688, x = context_layer_29)[name = tensor<string, []>("transpose_88")];
            tensor<fp32, [1, 512, 256]> input_171 = reshape(shape = var_693, x = transpose_88)[name = tensor<string, []>("input_171")];
            tensor<fp32, [1, 512, 256]> linear_46 = linear(bias = electra_encoder_layer_7_attention_output_dense_bias, weight = electra_encoder_layer_7_attention_output_dense_weight_affine_quantized, x = input_171)[name = tensor<string, []>("linear_46")];
            tensor<fp32, [1, 512, 256]> input_175 = add(x = linear_46, y = input_165)[name = tensor<string, []>("input_175")];
            tensor<int32, [1]> input_177_axes_0 = const()[name = tensor<string, []>("input_177_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp32, [1, 512, 256]> input_177 = layer_norm(axes = input_177_axes_0, beta = electra_encoder_layer_7_attention_output_LayerNorm_bias, epsilon = var_23, gamma = electra_encoder_layer_7_attention_output_LayerNorm_weight, x = input_175)[name = tensor<string, []>("input_177")];
            tensor<fp32, [1, 512, 1024]> linear_47 = linear(bias = electra_encoder_layer_7_intermediate_dense_bias, weight = electra_encoder_layer_7_intermediate_dense_weight_affine_quantized, x = input_177)[name = tensor<string, []>("linear_47")];
            tensor<string, []> input_181_mode_0 = const()[name = tensor<string, []>("input_181_mode_0"), val = tensor<string, []>("EXACT")];
            tensor<fp32, [1, 512, 1024]> input_181 = gelu(mode = input_181_mode_0, x = linear_47)[name = tensor<string, []>("input_181")];
            tensor<fp32, [1, 512, 256]> linear_48 = linear(bias = electra_encoder_layer_7_output_dense_bias, weight = electra_encoder_layer_7_output_dense_weight_affine_quantized, x = input_181)[name = tensor<string, []>("linear_48")];
            tensor<fp32, [1, 512, 256]> input_185 = add(x = linear_48, y = input_177)[name = tensor<string, []>("input_185")];
            tensor<int32, [1]> input_187_axes_0 = const()[name = tensor<string, []>("input_187_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp32, [1, 512, 256]> input_187 = layer_norm(axes = input_187_axes_0, beta = electra_encoder_layer_7_output_LayerNorm_bias, epsilon = var_23, gamma = electra_encoder_layer_7_output_LayerNorm_weight, x = input_185)[name = tensor<string, []>("input_187")];
            tensor<fp32, [1, 512, 256]> linear_49 = linear(bias = electra_encoder_layer_8_attention_self_query_bias, weight = electra_encoder_layer_8_attention_self_query_weight_affine_quantized, x = input_187)[name = tensor<string, []>("linear_49")];
            tensor<fp32, [1, 512, 256]> linear_50 = linear(bias = electra_encoder_layer_8_attention_self_key_bias, weight = electra_encoder_layer_8_attention_self_key_weight_affine_quantized, x = input_187)[name = tensor<string, []>("linear_50")];
            tensor<int32, [4]> var_738 = const()[name = tensor<string, []>("op_738"), val = tensor<int32, [4]>([1, 512, 4, 64])];
            tensor<fp32, [1, 512, 4, 64]> x_99 = reshape(shape = var_738, x = linear_50)[name = tensor<string, []>("x_99")];
            tensor<fp32, [1, 512, 256]> linear_51 = linear(bias = electra_encoder_layer_8_attention_self_value_bias, weight = electra_encoder_layer_8_attention_self_value_weight_affine_quantized, x = input_187)[name = tensor<string, []>("linear_51")];
            tensor<int32, [4]> var_747 = const()[name = tensor<string, []>("op_747"), val = tensor<int32, [4]>([1, 512, 4, 64])];
            tensor<fp32, [1, 512, 4, 64]> x_103 = reshape(shape = var_747, x = linear_51)[name = tensor<string, []>("x_103")];
            tensor<int32, [4]> var_749 = const()[name = tensor<string, []>("op_749"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [4]> var_753 = const()[name = tensor<string, []>("op_753"), val = tensor<int32, [4]>([1, 512, 4, 64])];
            tensor<fp32, [1, 512, 4, 64]> x_107 = reshape(shape = var_753, x = linear_49)[name = tensor<string, []>("x_107")];
            tensor<bool, []> attention_scores_33_transpose_x_0 = const()[name = tensor<string, []>("attention_scores_33_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> attention_scores_33_transpose_y_0 = const()[name = tensor<string, []>("attention_scores_33_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<int32, [4]> transpose_64_perm_0 = const()[name = tensor<string, []>("transpose_64_perm_0"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [4]> transpose_65_perm_0 = const()[name = tensor<string, []>("transpose_65_perm_0"), val = tensor<int32, [4]>([0, 2, 3, 1])];
            tensor<fp32, [1, 4, 64, 512]> transpose_86 = transpose(perm = transpose_65_perm_0, x = x_99)[name = tensor<string, []>("transpose_86")];
            tensor<fp32, [1, 4, 512, 64]> transpose_87 = transpose(perm = transpose_64_perm_0, x = x_107)[name = tensor<string, []>("transpose_87")];
            tensor<fp32, [1, 4, 512, 512]> attention_scores_33 = matmul(transpose_x = attention_scores_33_transpose_x_0, transpose_y = attention_scores_33_transpose_y_0, x = transpose_87, y = transpose_86)[name = tensor<string, []>("attention_scores_33")];
            tensor<fp32, []> _inversed_attention_scores_35_y_0 = const()[name = tensor<string, []>("_inversed_attention_scores_35_y_0"), val = tensor<fp32, []>(0x1p-3)];
            tensor<fp32, [1, 4, 512, 512]> _inversed_attention_scores_35 = mul(x = attention_scores_33, y = _inversed_attention_scores_35_y_0)[name = tensor<string, []>("_inversed_attention_scores_35")];
            tensor<fp32, [1, 4, 512, 512]> input_189 = add(x = _inversed_attention_scores_35, y = attention_mask_1)[name = tensor<string, []>("input_189")];
            tensor<fp32, [1, 4, 512, 512]> input_191 = softmax(axis = var_21, x = input_189)[name = tensor<string, []>("input_191")];
            tensor<bool, []> context_layer_33_transpose_x_0 = const()[name = tensor<string, []>("context_layer_33_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> context_layer_33_transpose_y_0 = const()[name = tensor<string, []>("context_layer_33_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<fp32, [1, 4, 512, 64]> transpose_85 = transpose(perm = var_749, x = x_103)[name = tensor<string, []>("transpose_85")];
            tensor<fp32, [1, 4, 512, 64]> context_layer_33 = matmul(transpose_x = context_layer_33_transpose_x_0, transpose_y = context_layer_33_transpose_y_0, x = input_191, y = transpose_85)[name = tensor<string, []>("context_layer_33")];
            tensor<int32, [4]> var_765 = const()[name = tensor<string, []>("op_765"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [3]> var_770 = const()[name = tensor<string, []>("op_770"), val = tensor<int32, [3]>([1, 512, 256])];
            tensor<fp32, [1, 512, 4, 64]> transpose_84 = transpose(perm = var_765, x = context_layer_33)[name = tensor<string, []>("transpose_84")];
            tensor<fp32, [1, 512, 256]> input_193 = reshape(shape = var_770, x = transpose_84)[name = tensor<string, []>("input_193")];
            tensor<fp32, [1, 512, 256]> linear_52 = linear(bias = electra_encoder_layer_8_attention_output_dense_bias, weight = electra_encoder_layer_8_attention_output_dense_weight_affine_quantized, x = input_193)[name = tensor<string, []>("linear_52")];
            tensor<fp32, [1, 512, 256]> input_197 = add(x = linear_52, y = input_187)[name = tensor<string, []>("input_197")];
            tensor<int32, [1]> input_199_axes_0 = const()[name = tensor<string, []>("input_199_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp32, [1, 512, 256]> input_199 = layer_norm(axes = input_199_axes_0, beta = electra_encoder_layer_8_attention_output_LayerNorm_bias, epsilon = var_23, gamma = electra_encoder_layer_8_attention_output_LayerNorm_weight, x = input_197)[name = tensor<string, []>("input_199")];
            tensor<fp32, [1, 512, 1024]> linear_53 = linear(bias = electra_encoder_layer_8_intermediate_dense_bias, weight = electra_encoder_layer_8_intermediate_dense_weight_affine_quantized, x = input_199)[name = tensor<string, []>("linear_53")];
            tensor<string, []> input_203_mode_0 = const()[name = tensor<string, []>("input_203_mode_0"), val = tensor<string, []>("EXACT")];
            tensor<fp32, [1, 512, 1024]> input_203 = gelu(mode = input_203_mode_0, x = linear_53)[name = tensor<string, []>("input_203")];
            tensor<fp32, [1, 512, 256]> linear_54 = linear(bias = electra_encoder_layer_8_output_dense_bias, weight = electra_encoder_layer_8_output_dense_weight_affine_quantized, x = input_203)[name = tensor<string, []>("linear_54")];
            tensor<fp32, [1, 512, 256]> input_207 = add(x = linear_54, y = input_199)[name = tensor<string, []>("input_207")];
            tensor<int32, [1]> input_209_axes_0 = const()[name = tensor<string, []>("input_209_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp32, [1, 512, 256]> input_209 = layer_norm(axes = input_209_axes_0, beta = electra_encoder_layer_8_output_LayerNorm_bias, epsilon = var_23, gamma = electra_encoder_layer_8_output_LayerNorm_weight, x = input_207)[name = tensor<string, []>("input_209")];
            tensor<fp32, [1, 512, 256]> linear_55 = linear(bias = electra_encoder_layer_9_attention_self_query_bias, weight = electra_encoder_layer_9_attention_self_query_weight_affine_quantized, x = input_209)[name = tensor<string, []>("linear_55")];
            tensor<fp32, [1, 512, 256]> linear_56 = linear(bias = electra_encoder_layer_9_attention_self_key_bias, weight = electra_encoder_layer_9_attention_self_key_weight_affine_quantized, x = input_209)[name = tensor<string, []>("linear_56")];
            tensor<int32, [4]> var_815 = const()[name = tensor<string, []>("op_815"), val = tensor<int32, [4]>([1, 512, 4, 64])];
            tensor<fp32, [1, 512, 4, 64]> x_111 = reshape(shape = var_815, x = linear_56)[name = tensor<string, []>("x_111")];
            tensor<fp32, [1, 512, 256]> linear_57 = linear(bias = electra_encoder_layer_9_attention_self_value_bias, weight = electra_encoder_layer_9_attention_self_value_weight_affine_quantized, x = input_209)[name = tensor<string, []>("linear_57")];
            tensor<int32, [4]> var_824 = const()[name = tensor<string, []>("op_824"), val = tensor<int32, [4]>([1, 512, 4, 64])];
            tensor<fp32, [1, 512, 4, 64]> x_115 = reshape(shape = var_824, x = linear_57)[name = tensor<string, []>("x_115")];
            tensor<int32, [4]> var_826 = const()[name = tensor<string, []>("op_826"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [4]> var_830 = const()[name = tensor<string, []>("op_830"), val = tensor<int32, [4]>([1, 512, 4, 64])];
            tensor<fp32, [1, 512, 4, 64]> x_119 = reshape(shape = var_830, x = linear_55)[name = tensor<string, []>("x_119")];
            tensor<bool, []> attention_scores_37_transpose_x_0 = const()[name = tensor<string, []>("attention_scores_37_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> attention_scores_37_transpose_y_0 = const()[name = tensor<string, []>("attention_scores_37_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<int32, [4]> transpose_66_perm_0 = const()[name = tensor<string, []>("transpose_66_perm_0"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [4]> transpose_67_perm_0 = const()[name = tensor<string, []>("transpose_67_perm_0"), val = tensor<int32, [4]>([0, 2, 3, 1])];
            tensor<fp32, [1, 4, 64, 512]> transpose_82 = transpose(perm = transpose_67_perm_0, x = x_111)[name = tensor<string, []>("transpose_82")];
            tensor<fp32, [1, 4, 512, 64]> transpose_83 = transpose(perm = transpose_66_perm_0, x = x_119)[name = tensor<string, []>("transpose_83")];
            tensor<fp32, [1, 4, 512, 512]> attention_scores_37 = matmul(transpose_x = attention_scores_37_transpose_x_0, transpose_y = attention_scores_37_transpose_y_0, x = transpose_83, y = transpose_82)[name = tensor<string, []>("attention_scores_37")];
            tensor<fp32, []> _inversed_attention_scores_39_y_0 = const()[name = tensor<string, []>("_inversed_attention_scores_39_y_0"), val = tensor<fp32, []>(0x1p-3)];
            tensor<fp32, [1, 4, 512, 512]> _inversed_attention_scores_39 = mul(x = attention_scores_37, y = _inversed_attention_scores_39_y_0)[name = tensor<string, []>("_inversed_attention_scores_39")];
            tensor<fp32, [1, 4, 512, 512]> input_211 = add(x = _inversed_attention_scores_39, y = attention_mask_1)[name = tensor<string, []>("input_211")];
            tensor<fp32, [1, 4, 512, 512]> input_213 = softmax(axis = var_21, x = input_211)[name = tensor<string, []>("input_213")];
            tensor<bool, []> context_layer_37_transpose_x_0 = const()[name = tensor<string, []>("context_layer_37_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> context_layer_37_transpose_y_0 = const()[name = tensor<string, []>("context_layer_37_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<fp32, [1, 4, 512, 64]> transpose_81 = transpose(perm = var_826, x = x_115)[name = tensor<string, []>("transpose_81")];
            tensor<fp32, [1, 4, 512, 64]> context_layer_37 = matmul(transpose_x = context_layer_37_transpose_x_0, transpose_y = context_layer_37_transpose_y_0, x = input_213, y = transpose_81)[name = tensor<string, []>("context_layer_37")];
            tensor<int32, [4]> var_842 = const()[name = tensor<string, []>("op_842"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [3]> var_847 = const()[name = tensor<string, []>("op_847"), val = tensor<int32, [3]>([1, 512, 256])];
            tensor<fp32, [1, 512, 4, 64]> transpose_80 = transpose(perm = var_842, x = context_layer_37)[name = tensor<string, []>("transpose_80")];
            tensor<fp32, [1, 512, 256]> input_215 = reshape(shape = var_847, x = transpose_80)[name = tensor<string, []>("input_215")];
            tensor<fp32, [1, 512, 256]> linear_58 = linear(bias = electra_encoder_layer_9_attention_output_dense_bias, weight = electra_encoder_layer_9_attention_output_dense_weight_affine_quantized, x = input_215)[name = tensor<string, []>("linear_58")];
            tensor<fp32, [1, 512, 256]> input_219 = add(x = linear_58, y = input_209)[name = tensor<string, []>("input_219")];
            tensor<int32, [1]> input_221_axes_0 = const()[name = tensor<string, []>("input_221_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp32, [1, 512, 256]> input_221 = layer_norm(axes = input_221_axes_0, beta = electra_encoder_layer_9_attention_output_LayerNorm_bias, epsilon = var_23, gamma = electra_encoder_layer_9_attention_output_LayerNorm_weight, x = input_219)[name = tensor<string, []>("input_221")];
            tensor<fp32, [1, 512, 1024]> linear_59 = linear(bias = electra_encoder_layer_9_intermediate_dense_bias, weight = electra_encoder_layer_9_intermediate_dense_weight_affine_quantized, x = input_221)[name = tensor<string, []>("linear_59")];
            tensor<string, []> input_225_mode_0 = const()[name = tensor<string, []>("input_225_mode_0"), val = tensor<string, []>("EXACT")];
            tensor<fp32, [1, 512, 1024]> input_225 = gelu(mode = input_225_mode_0, x = linear_59)[name = tensor<string, []>("input_225")];
            tensor<fp32, [1, 512, 256]> linear_60 = linear(bias = electra_encoder_layer_9_output_dense_bias, weight = electra_encoder_layer_9_output_dense_weight_affine_quantized, x = input_225)[name = tensor<string, []>("linear_60")];
            tensor<fp32, [1, 512, 256]> input_229 = add(x = linear_60, y = input_221)[name = tensor<string, []>("input_229")];
            tensor<int32, [1]> input_231_axes_0 = const()[name = tensor<string, []>("input_231_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp32, [1, 512, 256]> input_231 = layer_norm(axes = input_231_axes_0, beta = electra_encoder_layer_9_output_LayerNorm_bias, epsilon = var_23, gamma = electra_encoder_layer_9_output_LayerNorm_weight, x = input_229)[name = tensor<string, []>("input_231")];
            tensor<fp32, [1, 512, 256]> linear_61 = linear(bias = electra_encoder_layer_10_attention_self_query_bias, weight = electra_encoder_layer_10_attention_self_query_weight_affine_quantized, x = input_231)[name = tensor<string, []>("linear_61")];
            tensor<fp32, [1, 512, 256]> linear_62 = linear(bias = electra_encoder_layer_10_attention_self_key_bias, weight = electra_encoder_layer_10_attention_self_key_weight_affine_quantized, x = input_231)[name = tensor<string, []>("linear_62")];
            tensor<int32, [4]> var_892 = const()[name = tensor<string, []>("op_892"), val = tensor<int32, [4]>([1, 512, 4, 64])];
            tensor<fp32, [1, 512, 4, 64]> x_123 = reshape(shape = var_892, x = linear_62)[name = tensor<string, []>("x_123")];
            tensor<fp32, [1, 512, 256]> linear_63 = linear(bias = electra_encoder_layer_10_attention_self_value_bias, weight = electra_encoder_layer_10_attention_self_value_weight_affine_quantized, x = input_231)[name = tensor<string, []>("linear_63")];
            tensor<int32, [4]> var_901 = const()[name = tensor<string, []>("op_901"), val = tensor<int32, [4]>([1, 512, 4, 64])];
            tensor<fp32, [1, 512, 4, 64]> x_127 = reshape(shape = var_901, x = linear_63)[name = tensor<string, []>("x_127")];
            tensor<int32, [4]> var_903 = const()[name = tensor<string, []>("op_903"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [4]> var_907 = const()[name = tensor<string, []>("op_907"), val = tensor<int32, [4]>([1, 512, 4, 64])];
            tensor<fp32, [1, 512, 4, 64]> x_131 = reshape(shape = var_907, x = linear_61)[name = tensor<string, []>("x_131")];
            tensor<bool, []> attention_scores_41_transpose_x_0 = const()[name = tensor<string, []>("attention_scores_41_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> attention_scores_41_transpose_y_0 = const()[name = tensor<string, []>("attention_scores_41_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<int32, [4]> transpose_68_perm_0 = const()[name = tensor<string, []>("transpose_68_perm_0"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [4]> transpose_69_perm_0 = const()[name = tensor<string, []>("transpose_69_perm_0"), val = tensor<int32, [4]>([0, 2, 3, 1])];
            tensor<fp32, [1, 4, 64, 512]> transpose_78 = transpose(perm = transpose_69_perm_0, x = x_123)[name = tensor<string, []>("transpose_78")];
            tensor<fp32, [1, 4, 512, 64]> transpose_79 = transpose(perm = transpose_68_perm_0, x = x_131)[name = tensor<string, []>("transpose_79")];
            tensor<fp32, [1, 4, 512, 512]> attention_scores_41 = matmul(transpose_x = attention_scores_41_transpose_x_0, transpose_y = attention_scores_41_transpose_y_0, x = transpose_79, y = transpose_78)[name = tensor<string, []>("attention_scores_41")];
            tensor<fp32, []> _inversed_attention_scores_43_y_0 = const()[name = tensor<string, []>("_inversed_attention_scores_43_y_0"), val = tensor<fp32, []>(0x1p-3)];
            tensor<fp32, [1, 4, 512, 512]> _inversed_attention_scores_43 = mul(x = attention_scores_41, y = _inversed_attention_scores_43_y_0)[name = tensor<string, []>("_inversed_attention_scores_43")];
            tensor<fp32, [1, 4, 512, 512]> input_233 = add(x = _inversed_attention_scores_43, y = attention_mask_1)[name = tensor<string, []>("input_233")];
            tensor<fp32, [1, 4, 512, 512]> input_235 = softmax(axis = var_21, x = input_233)[name = tensor<string, []>("input_235")];
            tensor<bool, []> context_layer_41_transpose_x_0 = const()[name = tensor<string, []>("context_layer_41_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> context_layer_41_transpose_y_0 = const()[name = tensor<string, []>("context_layer_41_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<fp32, [1, 4, 512, 64]> transpose_77 = transpose(perm = var_903, x = x_127)[name = tensor<string, []>("transpose_77")];
            tensor<fp32, [1, 4, 512, 64]> context_layer_41 = matmul(transpose_x = context_layer_41_transpose_x_0, transpose_y = context_layer_41_transpose_y_0, x = input_235, y = transpose_77)[name = tensor<string, []>("context_layer_41")];
            tensor<int32, [4]> var_919 = const()[name = tensor<string, []>("op_919"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [3]> var_924 = const()[name = tensor<string, []>("op_924"), val = tensor<int32, [3]>([1, 512, 256])];
            tensor<fp32, [1, 512, 4, 64]> transpose_76 = transpose(perm = var_919, x = context_layer_41)[name = tensor<string, []>("transpose_76")];
            tensor<fp32, [1, 512, 256]> input_237 = reshape(shape = var_924, x = transpose_76)[name = tensor<string, []>("input_237")];
            tensor<fp32, [1, 512, 256]> linear_64 = linear(bias = electra_encoder_layer_10_attention_output_dense_bias, weight = electra_encoder_layer_10_attention_output_dense_weight_affine_quantized, x = input_237)[name = tensor<string, []>("linear_64")];
            tensor<fp32, [1, 512, 256]> input_241 = add(x = linear_64, y = input_231)[name = tensor<string, []>("input_241")];
            tensor<int32, [1]> input_243_axes_0 = const()[name = tensor<string, []>("input_243_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp32, [1, 512, 256]> input_243 = layer_norm(axes = input_243_axes_0, beta = electra_encoder_layer_10_attention_output_LayerNorm_bias, epsilon = var_23, gamma = electra_encoder_layer_10_attention_output_LayerNorm_weight, x = input_241)[name = tensor<string, []>("input_243")];
            tensor<fp32, [1, 512, 1024]> linear_65 = linear(bias = electra_encoder_layer_10_intermediate_dense_bias, weight = electra_encoder_layer_10_intermediate_dense_weight_affine_quantized, x = input_243)[name = tensor<string, []>("linear_65")];
            tensor<string, []> input_247_mode_0 = const()[name = tensor<string, []>("input_247_mode_0"), val = tensor<string, []>("EXACT")];
            tensor<fp32, [1, 512, 1024]> input_247 = gelu(mode = input_247_mode_0, x = linear_65)[name = tensor<string, []>("input_247")];
            tensor<fp32, [1, 512, 256]> linear_66 = linear(bias = electra_encoder_layer_10_output_dense_bias, weight = electra_encoder_layer_10_output_dense_weight_affine_quantized, x = input_247)[name = tensor<string, []>("linear_66")];
            tensor<fp32, [1, 512, 256]> input_251 = add(x = linear_66, y = input_243)[name = tensor<string, []>("input_251")];
            tensor<int32, [1]> input_253_axes_0 = const()[name = tensor<string, []>("input_253_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp32, [1, 512, 256]> input_253 = layer_norm(axes = input_253_axes_0, beta = electra_encoder_layer_10_output_LayerNorm_bias, epsilon = var_23, gamma = electra_encoder_layer_10_output_LayerNorm_weight, x = input_251)[name = tensor<string, []>("input_253")];
            tensor<fp32, [1, 512, 256]> linear_67 = linear(bias = electra_encoder_layer_11_attention_self_query_bias, weight = electra_encoder_layer_11_attention_self_query_weight_affine_quantized, x = input_253)[name = tensor<string, []>("linear_67")];
            tensor<fp32, [1, 512, 256]> linear_68 = linear(bias = electra_encoder_layer_11_attention_self_key_bias, weight = electra_encoder_layer_11_attention_self_key_weight_affine_quantized, x = input_253)[name = tensor<string, []>("linear_68")];
            tensor<int32, [4]> var_969 = const()[name = tensor<string, []>("op_969"), val = tensor<int32, [4]>([1, 512, 4, 64])];
            tensor<fp32, [1, 512, 4, 64]> x_135 = reshape(shape = var_969, x = linear_68)[name = tensor<string, []>("x_135")];
            tensor<fp32, [1, 512, 256]> linear_69 = linear(bias = electra_encoder_layer_11_attention_self_value_bias, weight = electra_encoder_layer_11_attention_self_value_weight_affine_quantized, x = input_253)[name = tensor<string, []>("linear_69")];
            tensor<int32, [4]> var_978 = const()[name = tensor<string, []>("op_978"), val = tensor<int32, [4]>([1, 512, 4, 64])];
            tensor<fp32, [1, 512, 4, 64]> x_139 = reshape(shape = var_978, x = linear_69)[name = tensor<string, []>("x_139")];
            tensor<int32, [4]> var_980 = const()[name = tensor<string, []>("op_980"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [4]> var_984 = const()[name = tensor<string, []>("op_984"), val = tensor<int32, [4]>([1, 512, 4, 64])];
            tensor<fp32, [1, 512, 4, 64]> x = reshape(shape = var_984, x = linear_67)[name = tensor<string, []>("x")];
            tensor<bool, []> attention_scores_45_transpose_x_0 = const()[name = tensor<string, []>("attention_scores_45_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> attention_scores_45_transpose_y_0 = const()[name = tensor<string, []>("attention_scores_45_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<int32, [4]> transpose_70_perm_0 = const()[name = tensor<string, []>("transpose_70_perm_0"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [4]> transpose_71_perm_0 = const()[name = tensor<string, []>("transpose_71_perm_0"), val = tensor<int32, [4]>([0, 2, 3, 1])];
            tensor<fp32, [1, 4, 64, 512]> transpose_74 = transpose(perm = transpose_71_perm_0, x = x_135)[name = tensor<string, []>("transpose_74")];
            tensor<fp32, [1, 4, 512, 64]> transpose_75 = transpose(perm = transpose_70_perm_0, x = x)[name = tensor<string, []>("transpose_75")];
            tensor<fp32, [1, 4, 512, 512]> attention_scores_45 = matmul(transpose_x = attention_scores_45_transpose_x_0, transpose_y = attention_scores_45_transpose_y_0, x = transpose_75, y = transpose_74)[name = tensor<string, []>("attention_scores_45")];
            tensor<fp32, []> _inversed_attention_scores_y_0 = const()[name = tensor<string, []>("_inversed_attention_scores_y_0"), val = tensor<fp32, []>(0x1p-3)];
            tensor<fp32, [1, 4, 512, 512]> _inversed_attention_scores = mul(x = attention_scores_45, y = _inversed_attention_scores_y_0)[name = tensor<string, []>("_inversed_attention_scores")];
            tensor<fp32, [1, 4, 512, 512]> input_255 = add(x = _inversed_attention_scores, y = attention_mask_1)[name = tensor<string, []>("input_255")];
            tensor<fp32, [1, 4, 512, 512]> input_257 = softmax(axis = var_21, x = input_255)[name = tensor<string, []>("input_257")];
            tensor<bool, []> context_layer_45_transpose_x_0 = const()[name = tensor<string, []>("context_layer_45_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> context_layer_45_transpose_y_0 = const()[name = tensor<string, []>("context_layer_45_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<fp32, [1, 4, 512, 64]> transpose_73 = transpose(perm = var_980, x = x_139)[name = tensor<string, []>("transpose_73")];
            tensor<fp32, [1, 4, 512, 64]> context_layer_45 = matmul(transpose_x = context_layer_45_transpose_x_0, transpose_y = context_layer_45_transpose_y_0, x = input_257, y = transpose_73)[name = tensor<string, []>("context_layer_45")];
            tensor<int32, [4]> var_996 = const()[name = tensor<string, []>("op_996"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [3]> var_1001 = const()[name = tensor<string, []>("op_1001"), val = tensor<int32, [3]>([1, 512, 256])];
            tensor<fp32, [1, 512, 4, 64]> transpose_72 = transpose(perm = var_996, x = context_layer_45)[name = tensor<string, []>("transpose_72")];
            tensor<fp32, [1, 512, 256]> input_259 = reshape(shape = var_1001, x = transpose_72)[name = tensor<string, []>("input_259")];
            tensor<fp32, [1, 512, 256]> linear_70 = linear(bias = electra_encoder_layer_11_attention_output_dense_bias, weight = electra_encoder_layer_11_attention_output_dense_weight_affine_quantized, x = input_259)[name = tensor<string, []>("linear_70")];
            tensor<fp32, [1, 512, 256]> input_263 = add(x = linear_70, y = input_253)[name = tensor<string, []>("input_263")];
            tensor<int32, [1]> input_265_axes_0 = const()[name = tensor<string, []>("input_265_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp32, [1, 512, 256]> input_265 = layer_norm(axes = input_265_axes_0, beta = electra_encoder_layer_11_attention_output_LayerNorm_bias, epsilon = var_23, gamma = electra_encoder_layer_11_attention_output_LayerNorm_weight, x = input_263)[name = tensor<string, []>("input_265")];
            tensor<fp32, [1, 512, 1024]> linear_71 = linear(bias = electra_encoder_layer_11_intermediate_dense_bias, weight = electra_encoder_layer_11_intermediate_dense_weight_affine_quantized, x = input_265)[name = tensor<string, []>("linear_71")];
            tensor<string, []> input_269_mode_0 = const()[name = tensor<string, []>("input_269_mode_0"), val = tensor<string, []>("EXACT")];
            tensor<fp32, [1, 512, 1024]> input_269 = gelu(mode = input_269_mode_0, x = linear_71)[name = tensor<string, []>("input_269")];
            tensor<fp32, [1, 512, 256]> linear_72 = linear(bias = electra_encoder_layer_11_output_dense_bias, weight = electra_encoder_layer_11_output_dense_weight_affine_quantized, x = input_269)[name = tensor<string, []>("linear_72")];
            tensor<fp32, [1, 512, 256]> input_273 = add(x = linear_72, y = input_265)[name = tensor<string, []>("input_273")];
            tensor<int32, [1]> input_285_axes_0 = const()[name = tensor<string, []>("input_285_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp32, [1, 512, 256]> input_285 = layer_norm(axes = input_285_axes_0, beta = electra_encoder_layer_11_output_LayerNorm_bias, epsilon = var_23, gamma = electra_encoder_layer_11_output_LayerNorm_weight, x = input_273)[name = tensor<string, []>("input_285")];
            tensor<int32, [3]> var_1037_begin_0 = const()[name = tensor<string, []>("op_1037_begin_0"), val = tensor<int32, [3]>([0, 0, 0])];
            tensor<int32, [3]> var_1037_end_0 = const()[name = tensor<string, []>("op_1037_end_0"), val = tensor<int32, [3]>([1, 1, 256])];
            tensor<bool, [3]> var_1037_end_mask_0 = const()[name = tensor<string, []>("op_1037_end_mask_0"), val = tensor<bool, [3]>([true, false, true])];
            tensor<bool, [3]> var_1037_squeeze_mask_0 = const()[name = tensor<string, []>("op_1037_squeeze_mask_0"), val = tensor<bool, [3]>([false, true, false])];
            tensor<fp32, [1, 256]> var_1037 = slice_by_index(begin = var_1037_begin_0, end = var_1037_end_0, end_mask = var_1037_end_mask_0, squeeze_mask = var_1037_squeeze_mask_0, x = input_285)[name = tensor<string, []>("op_1037")];
            tensor<fp32, [1, 128]> linear_73 = linear(bias = sequence_classifier_0_bias, weight = sequence_classifier_0_weight_affine_quantized, x = var_1037)[name = tensor<string, []>("linear_73")];
            tensor<fp32, [1, 128]> input_281 = relu(x = linear_73)[name = tensor<string, []>("input_281")];
            tensor<fp32, [1, 2]> linear_74 = linear(bias = sequence_classifier_3_bias, weight = sequence_classifier_3_weight, x = input_281)[name = tensor<string, []>("linear_74")];
            tensor<fp32, [1, 512, 128]> linear_75 = linear(bias = token_classifier_0_bias, weight = token_classifier_0_weight_affine_quantized, x = input_285)[name = tensor<string, []>("linear_75")];
            tensor<fp32, [1, 512, 128]> input_291 = relu(x = linear_75)[name = tensor<string, []>("input_291")];
            tensor<fp32, [1, 512, 3]> linear_76 = linear(bias = token_classifier_3_bias, weight = token_classifier_3_weight, x = input_291)[name = tensor<string, []>("linear_76")];
            tensor<int32, []> var_1073 = const()[name = tensor<string, []>("op_1073"), val = tensor<int32, []>(-1)];
            tensor<fp32, [1, 2]> sequence_classification = softmax(axis = var_1073, x = linear_74)[name = tensor<string, []>("op_1075")];
            tensor<int32, []> var_1076 = const()[name = tensor<string, []>("op_1076"), val = tensor<int32, []>(-1)];
            tensor<fp32, [1, 512, 3]> token_classification = softmax(axis = var_1076, x = linear_76)[name = tensor<string, []>("op_1078")];
        } -> (sequence_classification, token_classification);
}