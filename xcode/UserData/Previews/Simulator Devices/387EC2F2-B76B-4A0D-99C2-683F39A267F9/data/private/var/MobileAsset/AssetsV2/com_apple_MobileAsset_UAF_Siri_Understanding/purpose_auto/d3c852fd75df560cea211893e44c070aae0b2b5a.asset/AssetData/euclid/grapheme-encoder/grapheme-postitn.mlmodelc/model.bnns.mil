program(1.0)
[buildInfo = dict<tensor<string, []>, tensor<string, []>>({{"coremlc-component-MIL", "3304.6.1"}, {"coremlc-version", "3304.7.1"}})]
{
    func main<ios17>(tensor<fp32, [1, ?]> idx) [FlexibleShapeInformation = tuple<tuple<tensor<string, []>, dict<tensor<string, []>, tensor<int32, [?]>>>, tuple<tensor<string, []>, dict<tensor<string, []>, list<tensor<int32, [2]>, ?>>>>((("DefaultShapes", {{"idx", [1, 1]}}), ("RangeDims", {{"idx", [[1, 1], [1, 128]]}})))] {
            tensor<int32, []> var_11 = const()[name = tensor<string, []>("op_11"), val = tensor<int32, []>(256)];
            tensor<int32, []> var_16 = const()[name = tensor<string, []>("op_16"), val = tensor<int32, []>(1)];
            tensor<string, []> input_data0_1_dtype_0 = const()[name = tensor<string, []>("input_data0_1_dtype_0"), val = tensor<string, []>("int32")];
            tensor<int32, []> var_38_axis_0 = const()[name = tensor<string, []>("op_38_axis_0"), val = tensor<int32, []>(-1)];
            tensor<int32, []> var_38_on_value_0 = const()[name = tensor<string, []>("op_38_on_value_0"), val = tensor<int32, []>(1)];
            tensor<int32, []> var_38_off_value_0 = const()[name = tensor<string, []>("op_38_off_value_0"), val = tensor<int32, []>(0)];
            tensor<int32, [1, ?]> cast_2 = cast(dtype = input_data0_1_dtype_0, x = idx)[name = tensor<string, []>("cast_2")];
            tensor<int32, [1, ?, 256]> var_38 = one_hot(axis = var_38_axis_0, indices = cast_2, off_value = var_38_off_value_0, on_value = var_38_on_value_0, one_hot_vector_size = var_11)[name = tensor<string, []>("op_38")];
            tensor<fp16, [256]> op_7_to_fp16_affine_quantized = constexpr_affine_dequantize()[axis = tensor<int32, []>(0), name = tensor<string, []>("op_7_to_fp16_affine_quantized"), quantized_data = tensor<int8, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(64))), scale = tensor<fp16, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(704))), zero_point = tensor<int8, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(384)))];
            tensor<string, []> output_1_to_fp16_dtype_0 = const()[name = tensor<string, []>("output_1_to_fp16_dtype_0"), val = tensor<string, []>("fp16")];
            tensor<fp16, [1, ?, 256]> cast_1 = cast(dtype = output_1_to_fp16_dtype_0, x = var_38)[name = tensor<string, []>("cast_1")];
            tensor<fp16, [1, ?, 256]> var_40_cast_fp16 = mul(x = op_7_to_fp16_affine_quantized, y = cast_1)[name = tensor<string, []>("op_40_cast_fp16")];
            tensor<fp16, [256]> op_6_to_fp16_affine_quantized = constexpr_affine_dequantize()[axis = tensor<int32, []>(0), name = tensor<string, []>("op_6_to_fp16_affine_quantized"), quantized_data = tensor<int8, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(1280))), scale = tensor<fp16, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(1920))), zero_point = tensor<int8, [256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(1600)))];
            tensor<fp16, [1, ?, 256]> input_1_cast_fp16 = add(x = var_40_cast_fp16, y = op_6_to_fp16_affine_quantized)[name = tensor<string, []>("input_1_cast_fp16")];
            tensor<int32, [3]> input_1_batch_first_transpose_perm_0 = const()[name = tensor<string, []>("input_1_batch_first_transpose_perm_0"), val = tensor<int32, [3]>([1, 0, 2])];
            tensor<string, []> output0_1_batch_first_direction_0 = const()[name = tensor<string, []>("output0_1_batch_first_direction_0"), val = tensor<string, []>("bidirectional")];
            tensor<bool, []> output0_1_batch_first_output_sequence_0 = const()[name = tensor<string, []>("output0_1_batch_first_output_sequence_0"), val = tensor<bool, []>(true)];
            tensor<string, []> output0_1_batch_first_recurrent_activation_0 = const()[name = tensor<string, []>("output0_1_batch_first_recurrent_activation_0"), val = tensor<string, []>("sigmoid")];
            tensor<string, []> output0_1_batch_first_cell_activation_0 = const()[name = tensor<string, []>("output0_1_batch_first_cell_activation_0"), val = tensor<string, []>("tanh")];
            tensor<string, []> output0_1_batch_first_activation_0 = const()[name = tensor<string, []>("output0_1_batch_first_activation_0"), val = tensor<string, []>("tanh")];
            tensor<fp16, [1, 600]> output0_1_batch_first_lstm_h0_reshaped_to_fp16_affine_quantized = constexpr_affine_dequantize()[axis = tensor<int32, []>(0), name = tensor<string, []>("output0_1_batch_first_lstm_h0_reshaped_to_fp16_affine_quantized"), quantized_data = tensor<int8, [1, 600]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(2496))), scale = tensor<fp16, []>(0x0p+0), zero_point = tensor<int8, []>(0)];
            tensor<fp16, [1200, 256]> concat_4_to_fp16_affine_quantized = constexpr_affine_dequantize()[axis = tensor<int32, []>(0), name = tensor<string, []>("concat_4_to_fp16_affine_quantized"), quantized_data = tensor<int8, [1200, 256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(3200))), scale = tensor<fp16, [1200]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(311744))), zero_point = tensor<int8, [1200]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(310464)))];
            tensor<fp16, [1200, 300]> concat_5_to_fp16_affine_quantized = constexpr_affine_dequantize()[axis = tensor<int32, []>(0), name = tensor<string, []>("concat_5_to_fp16_affine_quantized"), quantized_data = tensor<int8, [1200, 300]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(314240))), scale = tensor<fp16, [1200]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(675584))), zero_point = tensor<int8, [1200]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(674304)))];
            tensor<fp16, [1200]> add_1_to_fp16_affine_quantized = constexpr_affine_dequantize()[axis = tensor<int32, []>(0), name = tensor<string, []>("add_1_to_fp16_affine_quantized"), quantized_data = tensor<int8, [1200]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(678080))), scale = tensor<fp16, [1200]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(680640))), zero_point = tensor<int8, [1200]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(679360)))];
            tensor<fp16, [1200, 256]> concat_6_to_fp16_affine_quantized = constexpr_affine_dequantize()[axis = tensor<int32, []>(0), name = tensor<string, []>("concat_6_to_fp16_affine_quantized"), quantized_data = tensor<int8, [1200, 256]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(683136))), scale = tensor<fp16, [1200]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(991680))), zero_point = tensor<int8, [1200]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(990400)))];
            tensor<fp16, [1200, 300]> concat_7_to_fp16_affine_quantized = constexpr_affine_dequantize()[axis = tensor<int32, []>(0), name = tensor<string, []>("concat_7_to_fp16_affine_quantized"), quantized_data = tensor<int8, [1200, 300]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(994176))), scale = tensor<fp16, [1200]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(1355520))), zero_point = tensor<int8, [1200]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(1354240)))];
            tensor<fp16, [1200]> add_2_to_fp16_affine_quantized = constexpr_affine_dequantize()[axis = tensor<int32, []>(0), name = tensor<string, []>("add_2_to_fp16_affine_quantized"), quantized_data = tensor<int8, [1200]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(1358016))), scale = tensor<fp16, [1200]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(1360576))), zero_point = tensor<int8, [1200]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(1359296)))];
            tensor<fp16, [?, 1, 256]> transpose_1 = transpose(perm = input_1_batch_first_transpose_perm_0, x = input_1_cast_fp16)[name = tensor<string, []>("transpose_1")];
            tensor<fp16, [?, 1, 600]> output0_1_batch_first_cast_fp16_0, tensor<fp16, [1, 600]> output0_1_batch_first_cast_fp16_1, tensor<fp16, [1, 600]> output0_1_batch_first_cast_fp16_2 = lstm(activation = output0_1_batch_first_activation_0, bias = add_1_to_fp16_affine_quantized, bias_back = add_2_to_fp16_affine_quantized, cell_activation = output0_1_batch_first_cell_activation_0, direction = output0_1_batch_first_direction_0, initial_c = output0_1_batch_first_lstm_h0_reshaped_to_fp16_affine_quantized, initial_h = output0_1_batch_first_lstm_h0_reshaped_to_fp16_affine_quantized, output_sequence = output0_1_batch_first_output_sequence_0, recurrent_activation = output0_1_batch_first_recurrent_activation_0, weight_hh = concat_5_to_fp16_affine_quantized, weight_hh_back = concat_7_to_fp16_affine_quantized, weight_ih = concat_4_to_fp16_affine_quantized, weight_ih_back = concat_6_to_fp16_affine_quantized, x = transpose_1)[name = tensor<string, []>("output0_1_batch_first_cast_fp16")];
            tensor<int32, [3]> output0_1_perm_0 = const()[name = tensor<string, []>("output0_1_perm_0"), val = tensor<int32, [3]>([1, 0, 2])];
            tensor<int32, [3]> var_56_begin_0 = const()[name = tensor<string, []>("op_56_begin_0"), val = tensor<int32, [3]>([0, -1, 0])];
            tensor<int32, [3]> var_56_end_0 = const()[name = tensor<string, []>("op_56_end_0"), val = tensor<int32, [3]>([1, 0, 600])];
            tensor<bool, [3]> var_56_end_mask_0 = const()[name = tensor<string, []>("op_56_end_mask_0"), val = tensor<bool, [3]>([true, true, true])];
            tensor<bool, [3]> var_56_squeeze_mask_0 = const()[name = tensor<string, []>("op_56_squeeze_mask_0"), val = tensor<bool, [3]>([false, true, false])];
            tensor<fp16, [1, ?, 600]> transpose_0 = transpose(perm = output0_1_perm_0, x = output0_1_batch_first_cast_fp16_0)[name = tensor<string, []>("transpose_0")];
            tensor<fp16, [1, 600]> var_56_cast_fp16 = slice_by_index(begin = var_56_begin_0, end = var_56_end_0, end_mask = var_56_end_mask_0, squeeze_mask = var_56_squeeze_mask_0, x = transpose_0)[name = tensor<string, []>("op_56_cast_fp16")];
            tensor<int32, [2]> var_57_begin_0 = const()[name = tensor<string, []>("op_57_begin_0"), val = tensor<int32, [2]>([0, 0])];
            tensor<int32, [2]> var_57_end_0 = const()[name = tensor<string, []>("op_57_end_0"), val = tensor<int32, [2]>([1, 300])];
            tensor<bool, [2]> var_57_end_mask_0 = const()[name = tensor<string, []>("op_57_end_mask_0"), val = tensor<bool, [2]>([true, false])];
            tensor<fp16, [1, 300]> var_57_cast_fp16 = slice_by_index(begin = var_57_begin_0, end = var_57_end_0, end_mask = var_57_end_mask_0, x = var_56_cast_fp16)[name = tensor<string, []>("op_57_cast_fp16")];
            tensor<int32, [3]> var_59_begin_0 = const()[name = tensor<string, []>("op_59_begin_0"), val = tensor<int32, [3]>([0, 0, 0])];
            tensor<int32, [3]> var_59_end_0 = const()[name = tensor<string, []>("op_59_end_0"), val = tensor<int32, [3]>([1, 1, 600])];
            tensor<bool, [3]> var_59_end_mask_0 = const()[name = tensor<string, []>("op_59_end_mask_0"), val = tensor<bool, [3]>([true, false, true])];
            tensor<bool, [3]> var_59_squeeze_mask_0 = const()[name = tensor<string, []>("op_59_squeeze_mask_0"), val = tensor<bool, [3]>([false, true, false])];
            tensor<fp16, [1, 600]> var_59_cast_fp16 = slice_by_index(begin = var_59_begin_0, end = var_59_end_0, end_mask = var_59_end_mask_0, squeeze_mask = var_59_squeeze_mask_0, x = transpose_0)[name = tensor<string, []>("op_59_cast_fp16")];
            tensor<int32, [2]> var_60_begin_0 = const()[name = tensor<string, []>("op_60_begin_0"), val = tensor<int32, [2]>([0, 300])];
            tensor<int32, [2]> var_60_end_0 = const()[name = tensor<string, []>("op_60_end_0"), val = tensor<int32, [2]>([1, 600])];
            tensor<bool, [2]> var_60_end_mask_0 = const()[name = tensor<string, []>("op_60_end_mask_0"), val = tensor<bool, [2]>([true, true])];
            tensor<fp16, [1, 300]> var_60_cast_fp16 = slice_by_index(begin = var_60_begin_0, end = var_60_end_0, end_mask = var_60_end_mask_0, x = var_59_cast_fp16)[name = tensor<string, []>("op_60_cast_fp16")];
            tensor<bool, []> output1_1_interleave_0 = const()[name = tensor<string, []>("output1_1_interleave_0"), val = tensor<bool, []>(false)];
            tensor<fp16, [1, 600]> output1_1_cast_fp16 = concat(axis = var_16, interleave = output1_1_interleave_0, values = (var_57_cast_fp16, var_60_cast_fp16))[name = tensor<string, []>("output1_1_cast_fp16")];
            tensor<fp16, [40, 600]> op_66_weight_0_to_fp16_affine_quantized = constexpr_affine_dequantize()[axis = tensor<int32, []>(0), name = tensor<string, []>("op_66_weight_0_to_fp16_affine_quantized"), quantized_data = tensor<int8, [40, 600]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(1363072))), scale = tensor<fp16, [40]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(1387264))), zero_point = tensor<int8, [40]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(1387136)))];
            tensor<fp16, [40]> op_66_bias_0_to_fp16_affine_quantized = constexpr_affine_dequantize()[axis = tensor<int32, []>(0), name = tensor<string, []>("op_66_bias_0_to_fp16_affine_quantized"), quantized_data = tensor<int8, [40]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(1387456))), scale = tensor<fp16, [40]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(1387712))), zero_point = tensor<int8, [40]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(1387584)))];
            tensor<fp16, [1, 40]> var_66_cast_fp16 = linear(bias = op_66_bias_0_to_fp16_affine_quantized, weight = op_66_weight_0_to_fp16_affine_quantized, x = output1_1_cast_fp16)[name = tensor<string, []>("op_66_cast_fp16")];
            tensor<fp16, [40]> _inversed_67_y_0_to_fp16_affine_quantized = constexpr_affine_dequantize()[axis = tensor<int32, []>(0), name = tensor<string, []>("_inversed_67_y_0_to_fp16_affine_quantized"), quantized_data = tensor<int8, [40]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(1387904))), scale = tensor<fp16, [40]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(1388160))), zero_point = tensor<int8, [40]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(1388032)))];
            tensor<fp16, [1, 40]> _inversed_67_cast_fp16 = mul(x = var_66_cast_fp16, y = _inversed_67_y_0_to_fp16_affine_quantized)[name = tensor<string, []>("_inversed_67_cast_fp16")];
            tensor<string, []> _inversed_67_cast_fp16_to_fp32_dtype_0 = const()[name = tensor<string, []>("_inversed_67_cast_fp16_to_fp32_dtype_0"), val = tensor<string, []>("fp32")];
            tensor<fp32, [1, 40]> embed = cast(dtype = _inversed_67_cast_fp16_to_fp32_dtype_0, x = _inversed_67_cast_fp16)[name = tensor<string, []>("cast_0")];
        } -> (embed);
}